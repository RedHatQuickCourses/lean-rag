= Techniques for Model Compression: Quantization (FP8, INT8) and Sparsity

== Introduction

Model compression is essential for deploying LLMs in production. By reducing model size and memory requirements, you can:
- Serve models on smaller/cheaper GPUs
- Increase throughput (more concurrent requests)
- Reduce infrastructure costs by 50%+
- Maintain >95% accuracy in most cases

This section covers quantization and sparsity techniques used in the Lean RAG Accelerator.

== Understanding Quantization

=== What is Quantization?

Quantization converts high-precision floating-point numbers to lower-precision formats, reducing memory footprint and accelerating computation.

**Precision Levels:**
- **FP32** (32-bit float): Full precision, baseline
- **FP16** (16-bit float): Half precision, 2x memory reduction
- **FP8** (8-bit float): Quarter precision, 4x memory reduction
- **INT8** (8-bit integer): Integer precision, 4x memory reduction
- **INT4** (4-bit integer): Very low precision, 8x memory reduction

=== Quantization Trade-offs

| Precision | Memory Reduction | Speed Improvement | Accuracy Retention | Use Case |
|-----------|-----------------|-------------------|-------------------|----------|
| FP32 | Baseline | Baseline | 100% | Training, research |
| FP16 | 2x | 1.5x | ~99% | Standard inference |
| FP8 | 4x | 2x | ~98% | Production (new) |
| INT8 | 4x | 2x | ~95% | Production (common) |
| INT4 | 8x | 3x | ~90% | Resource-constrained |

== Quantization Methods

=== FP8 (8-bit Floating Point)

**Characteristics:**
- Latest precision format (2023+)
- Maintains floating-point representation
- Better accuracy retention than INT8
- Hardware support in newer GPUs (H100, A100)

**Advantages:**
- Best accuracy retention among 8-bit formats
- Natural fit for LLM activations
- Minimal calibration required

**Limitations:**
- Requires newer GPU hardware
- Less widely supported than INT8

**Use Cases:**
- Production deployments with modern GPUs
- When accuracy is critical
- New deployments (future-proof)

**Expected Results:**
- 4x memory reduction
- 2x inference speed
- 98%+ accuracy retention

=== INT8 (8-bit Integer)

**Characteristics:**
- Most widely supported quantization format
- Integer representation (no floating point)
- Requires calibration dataset
- Well-established tooling

**Advantages:**
- Broad hardware support
- Mature tooling (SmoothQuant, GPTQ)
- Excellent memory savings
- Good accuracy retention

**Limitations:**
- Requires calibration dataset
- Slightly lower accuracy than FP8
- May need fine-tuning for some models

**Use Cases:**
- Production deployments (most common)
- Cost-sensitive applications
- Standard GPU hardware

**Expected Results:**
- 4x memory reduction
- 2x inference speed
- 95%+ accuracy retention

**Quantization Algorithms for INT8:**

==== SmoothQuant

**How it works:**
- Smooths activation outliers before quantization
- Migrates quantization difficulty from activations to weights
- Post-training quantization (no retraining needed)

**Advantages:**
- No model retraining required
- Good accuracy retention
- Works well with most models

**Configuration:**
- Alpha parameter (0.0-1.0): Controls smoothing
- Calibration dataset: 512-1024 samples typical
- Batch size: 8-16 for calibration

**Best for:**
- General-purpose quantization
- Production deployments
- Llama models

==== GPTQ (GPT Quantization)

**How it works:**
- Layer-wise quantization with error correction
- More aggressive compression
- Typically used for INT4, but supports INT8

**Advantages:**
- Maximum compression
- Good for resource-constrained environments
- Can achieve INT4 precision

**Limitations:**
- More complex calibration
- Longer quantization time
- May require more calibration samples

**Best for:**
- Maximum compression needs
- INT4 quantization
- Resource-constrained deployments

=== INT4 (4-bit Integer)

**Characteristics:**
- Maximum compression (8x memory reduction)
- Significant accuracy trade-off
- Requires careful calibration

**Advantages:**
- Maximum memory savings
- Can run large models on smaller GPUs
- Good for edge deployments

**Limitations:**
- Lower accuracy retention (~90%)
- Requires extensive calibration
- May need model-specific tuning

**Use Cases:**
- Resource-constrained environments
- Edge deployments
- When model size is critical

**Expected Results:**
- 8x memory reduction
- 3x inference speed
- 90%+ accuracy retention

== Sparsity Techniques

Sparsity introduces zero values into weight matrices, reducing the number of non-zero parameters.

=== Structured Sparsity

**Approach:**
- Prunes entire neurons, filters, or layers
- Results in regular, hardware-friendly patterns
- Easier to accelerate on specialized hardware

**Advantages:**
- Hardware-friendly (efficient on GPUs)
- Predictable performance
- Easier to implement

**Limitations:**
- Less flexible than unstructured
- May remove important structures
- Requires careful selection of what to prune

**Use Cases:**
- Hardware-accelerated inference
- When regularity is important
- Production deployments

=== Unstructured Sparsity

**Approach:**
- Prunes individual weights within matrices
- Results in irregular, sparse patterns
- More flexible but harder to accelerate

**Advantages:**
- Maximum flexibility
- Can achieve higher sparsity rates
- Better accuracy retention

**Limitations:**
- Harder to accelerate (requires specialized hardware)
- Irregular patterns
- More complex implementation

**Use Cases:**
- Research and experimentation
- When maximum sparsity is needed
- Specialized hardware deployments

== Combining Techniques

=== Quantization + Sparsity

**Strategy:**
- Apply quantization first (INT8)
- Then apply sparsity (structured or unstructured)
- Can achieve 10x+ total compression

**Expected Results:**
- 4x from quantization (INT8)
- 2-3x from sparsity
- Total: 8-12x compression
- Accuracy: 90-95% retention

**Use Cases:**
- Extreme resource constraints
- Edge deployments
- Maximum cost optimization

== Real-World Examples

=== Example 1: Llama 3.1 8B with INT8 SmoothQuant

**Baseline:**
- Model: Llama 3.1 8B FP16
- Size: ~16GB
- GPU: Requires A10G (24GB) or larger

**After INT8 SmoothQuant:**
- Model: Llama 3.1 8B INT8
- Size: ~8GB (50% reduction)
- GPU: Can run on L4 (24GB) with room for batching
- Accuracy: 95%+ retention
- Speed: 2x faster inference

=== Example 2: Llama 3.2 3B with INT8

**Baseline:**
- Model: Llama 3.2 3B FP16
- Size: ~6GB

**After INT8:**
- Model: Llama 3.2 3B INT8
- Size: ~3GB (50% reduction)
- Can run on CPU or smaller GPUs
- Good for edge deployments

== Choosing the Right Technique

=== Decision Framework

**Consider your requirements:**

1. **Accuracy Requirements**
   - Critical (>98%): Use FP8 or FP16
   - High (>95%): Use INT8 SmoothQuant
   - Acceptable (>90%): Use INT8 GPTQ or INT4

2. **Hardware Constraints**
   - Modern GPUs (H100, A100): FP8 available
   - Standard GPUs (A10G, L4): INT8 recommended
   - Resource-constrained: INT4 or INT8 + Sparsity

3. **Use Case**
   - Production RAG: INT8 SmoothQuant
   - Real-time chat: INT8 or FP8
   - Edge deployment: INT4 or INT8 + Sparsity

4. **Cost Sensitivity**
   - High: INT8 or INT4
   - Medium: FP8 or INT8
   - Low: FP16 (baseline)

=== Recommended Approach for Lean RAG Accelerator

**For most RAG deployments:**
- **Method**: INT8 SmoothQuant
- **Model**: Llama 3.1 8B
- **Result**: 50% memory reduction, 95%+ accuracy retention
- **Implementation**: LLM Compressor (see next section)

== Key Takeaways

- Quantization reduces memory by 2-8x with minimal accuracy loss
- INT8 SmoothQuant is the recommended approach for most deployments
- FP8 offers best accuracy but requires newer hardware
- INT4 provides maximum compression but lower accuracy
- Sparsity can be combined with quantization for extreme compression
- 95%+ accuracy retention is achievable with proper quantization

== Next Steps

- Learn how to use LLM Compressor to apply quantization
- Understand quantization recipe configuration
- See hands-on examples in Module 2 labs

[NOTE]
====
**Documentation Gaps**: This section would benefit from:
- Visual diagram showing precision levels (FP32 â†’ INT4)
- Comparison charts of accuracy vs. memory for different methods
- Real benchmark results from quantized models
- Decision tree diagram for choosing quantization method
====