= Module 1: Inference Economics & RHOAI 3.0 Architecture

== Introduction

This module establishes the foundation for understanding why optimization matters and how Red Hat OpenShift AI (RHOAI) 3.0 addresses the critical challenges of LLM inference economics. You'll learn about the fundamental trade-offs in LLM deployments and how RHOAI 3.0's AI Factory approach provides solutions.

**Key Learning Objectives:**
- Understand the "Impossible Trinity" trade-offs (Accuracy, Latency, Cost)
- Identify GPU underutilization patterns and their cost impact
- Learn about RHOAI 3.0's AI Factory approach
- Map the RHOAI 3.0 inference stack components

== Module Overview

Most GenAI deployments fail not because the model isn't smart, but because it becomes too expensive to run at scale. This module explains why and introduces RHOAI 3.0's solutions.

=== The Problem

- **<40% GPU Utilization**: Most deployments waste 60% of expensive GPU capacity
- **Runaway Inference Economics**: 95% of AI pilots fail due to cost
- **Memory Bottlenecks**: KV cache inefficiency prevents efficient batching
- **Trade-off Decisions**: Must balance accuracy, latency, and cost

=== The Solution: RHOAI 3.0 AI Factory

RHOAI 3.0's AI Factory approach provides:
- **Industrialized Model Serving**: Standardized components and workflows
- **Optimization Tools**: LLM Compressor for model quantization
- **High-Performance Inference**: vLLM and llm-d for maximum throughput
- **Standardized RAG**: Llama Stack for consistent deployments

== Topics Covered

=== Section 1.1: The Impossible Trinity

Learn about the fundamental constraint in LLM deployments: you cannot simultaneously optimize accuracy, latency, and cost. This section covers:
- The three competing objectives
- Real-world trade-off scenarios
- Decision frameworks
- Optimization strategies

**Duration**: 30-45 minutes

=== Section 1.2: GPU Underutilization & Memory Management

Understand why most deployments use <40% of GPU capacity and how memory management impacts costs:
- KV cache bottleneck analysis
- Memory fragmentation problems
- Cost calculation examples
- Optimization techniques

**Duration**: 45-60 minutes

=== Section 1.3: RHOAI 3.0 AI Factory Approach

Explore Red Hat's industrialized model serving methodology:
- AI Factory principles
- Standardized components
- Automated workflows
- Benefits over ad-hoc deployments

**Duration**: 45-60 minutes

=== Section 1.4: RHOAI 3.0 Inference Stack

Map the inference stack components:
- vLLM: High-performance inference engine
- llm-d: Cloud-native orchestrator
- KServe: Kubernetes-native serving framework
- Integration and data flow

**Duration**: 60-75 minutes

== Prerequisites

Before starting this module, ensure you have:
- Basic understanding of LLMs and inference
- Familiarity with Kubernetes concepts
- Access to RHOAI 3.0 documentation (for reference)

== Expected Outcomes

By the end of this module, you will be able to:
- Explain the Impossible Trinity trade-offs
- Calculate the cost of GPU underutilization
- Describe RHOAI 3.0's AI Factory approach
- Map the inference stack components
- Make informed decisions about optimization strategies

== Module Structure

This module follows a conceptual learning path:

1. **Problem Understanding**: Learn why optimization matters
2. **Solution Introduction**: Understand RHOAI 3.0's approach
3. **Architecture Mapping**: Explore the inference stack
4. **Decision Framework**: Apply concepts to real scenarios

== Next Steps

After completing this module:
- Proceed to Module 2 to learn about model optimization techniques
- Review the business value documentation for ROI analysis
- Explore the quickstart code to see concepts in practice

== Related Resources

- link:../../lean-rag-accelerator/docs/business-value-driver.md[Business Value Driver] - ROI and problem statement
- link:../../lean-rag-accelerator/docs/architecture.md[Architecture Documentation] - Technical architecture
- link:module-2-model-optimization-with-llm-compressor.adoc[Module 2: Model Optimization] - Next module