= Identifying "Critical GPU Underutilization" and the Role of Memory Management in Inference Costs

== Introduction

**The Problem**: Most GenAI deployments utilize less than 40% of expensive GPU capacity, meaning you're paying for 60% idle resources. This "Critical GPU Underutilization" is the primary driver of runaway inference costs that kill 95% of AI pilots.

This section explains why GPU underutilization occurs, how memory management impacts costs, and how to identify and address these issues.

== Understanding GPU Underutilization

=== What is GPU Underutilization?

GPU underutilization occurs when GPU resources are not fully exploited during inference. This manifests as:

- **Low GPU Compute Utilization**: GPU cores sitting idle
- **Low Memory Utilization**: GPU memory allocated but unused
- **Inefficient Batching**: Processing requests sequentially instead of in parallel
- **Memory Fragmentation**: Available memory exists but cannot be allocated

=== The Current State: <40% Utilization

Industry benchmarks show typical LLM deployments achieve:

- **GPU Compute Utilization**: 30-40% average
- **GPU Memory Utilization**: 50-60% (but fragmented)
- **Effective Throughput**: 5-10 requests/second per GPU
- **Idle Time**: 60% of GPU cycles wasted

**The Cost Impact:**
- Paying for 2.5x more capacity than you use
- $10,000/month GPU costs → effectively $25,000/month value
- Cannot scale economically to production workloads

=== Why Underutilization Occurs

==== 1. KV Cache Memory Bottleneck

**The Problem:**
- Each request requires Key-Value (KV) cache for attention mechanism
- KV cache grows with context length and batch size
- Memory fragmentation prevents efficient batching
- Standard runtimes cannot efficiently manage KV cache

**Example:**
- Llama 3.1 8B with 8K context
- KV cache per request: ~2GB
- GPU memory: 24GB total
- Model weights: ~16GB
- Available for KV cache: ~8GB
- **Result**: Can only batch 3-4 requests, leaving GPU underutilized

==== 2. Inefficient Batching

**The Problem:**
- Standard runtimes process requests sequentially
- Cannot dynamically batch requests of different sizes
- Memory pre-allocation prevents flexible batching
- No continuous batching support

**Impact:**
- GPU waits for requests instead of processing continuously
- Memory reserved but unused between requests
- Throughput limited by request arrival rate

==== 3. Memory Fragmentation

**The Problem:**
- Fixed-size memory blocks allocated per request
- Blocks cannot be reused efficiently
- Fragmentation prevents larger batches
- Memory appears "full" but is actually fragmented

**Visual Example:**
```
GPU Memory (24GB):
[Model: 16GB][Request1: 2GB][Free: 1GB][Request2: 2GB][Free: 1GB][Request3: 2GB]
                    ↑ Fragmented - cannot allocate 2GB block for Request4
```

==== 4. Suboptimal Configuration

**Common Issues:**
- Conservative memory settings (e.g., `--gpu-memory-utilization=0.5`)
- No continuous batching enabled
- Sequential request processing
- No prefix caching for repeated prompts

== The Role of Memory Management in Inference Costs

Memory management is the **primary bottleneck** in LLM inference costs. Understanding this is critical for optimization.

=== Memory Components in LLM Inference

==== 1. Model Weights

**Size**: Largest component
- Llama 3.1 8B FP16: ~16GB
- Llama 3.1 8B INT8: ~8GB (50% reduction)
- Llama 3.1 8B INT4: ~4GB (75% reduction)

**Impact on Cost:**
- Larger models require more expensive GPUs
- Multiple GPUs needed for large models
- Quantization directly reduces this cost

==== 2. KV Cache (Key-Value Cache)

**Purpose**: Stores attention states for each token
**Size**: Grows with context length and batch size
- Per request: ~2GB for 8K context
- For 10 concurrent requests: ~20GB

**The Critical Bottleneck:**
- KV cache limits concurrent requests
- Fragmentation prevents efficient batching
- This is where most optimization gains occur

**Optimization Impact:**
- FP8 KV cache: 50% memory reduction
- PagedAttention: Eliminates fragmentation
- Prefix caching: Reuse cache for repeated prompts

==== 3. Activation Memory

**Purpose**: Intermediate computation results
**Size**: Smaller, but still significant
**Impact**: Affects batch size limits

=== Memory Management Techniques

==== 1. Quantization

**Technique**: Reduce precision of model weights and activations

**Impact:**
- FP16 → INT8: 50% memory reduction
- FP16 → INT4: 75% memory reduction
- Enables 2x-4x more concurrent requests

**Cost Savings:**
- 50% reduction in GPU memory requirements
- Can use smaller/cheaper GPUs
- Or serve 2x requests on same GPU

==== 2. PagedAttention

**Technique**: Manage KV cache like OS virtual memory

**How it works:**
- Divides KV cache into fixed-size pages
- Pages can be allocated/deallocated dynamically
- Eliminates memory fragmentation
- Enables efficient continuous batching

**Impact:**
- 2-4x throughput improvement
- Better GPU utilization (40% → 85-90%)
- No accuracy degradation

==== 3. Continuous Batching

**Technique**: Dynamically batch requests as they arrive

**How it works:**
- Add new requests to batch as GPU processes
- Remove completed requests from batch
- Keep GPU constantly busy

**Impact:**
- 2-3x throughput improvement
- Better GPU utilization
- Lower latency (no waiting for batch to fill)

==== 4. Prefix Caching

**Technique**: Reuse KV cache for repeated prompt prefixes

**How it works:**
- Cache KV cache for system prompts
- Reuse cache when same prefix appears
- Reduces computation for repeated content

**Impact:**
- 30-50% latency reduction for RAG systems
- Lower compute costs
- Better throughput

== Identifying GPU Underutilization

=== Monitoring GPU Utilization

**Tools:**
- `nvidia-smi`: Real-time GPU metrics
- Prometheus/Grafana: Historical trends
- RHOAI 3.0 monitoring: Integrated observability

**Key Metrics:**
- GPU Utilization (%): Should be >80%
- Memory Utilization (%): Should be >85%
- Memory Allocated vs. Used: Check for fragmentation
- Throughput (RPS): Compare to theoretical maximum

=== Baseline Measurement

**Step 1: Measure Current State**

[source,bash]
----
# Monitor GPU utilization
watch -n 1 nvidia-smi

# Check memory usage
nvidia-smi --query-gpu=memory.used,memory.total --format=csv

# Measure throughput
# Use your load testing tool (e.g., GuideLLM)
----

**Step 2: Calculate Utilization**

[source]
----
GPU Utilization = (Active Time / Total Time) × 100%
Target: >85%
Typical: <40% (problem!)

Memory Utilization = (Used Memory / Total Memory) × 100%
Target: >85%
Typical: 50-60% (fragmented)
----

**Step 3: Identify Bottlenecks**

- Low GPU utilization + High memory usage = Memory bottleneck
- Low GPU utilization + Low memory usage = Batching issue
- High GPU utilization + Low throughput = Configuration issue

=== Cost Calculation

**Calculate Cost of Underutilization:**

[source]
----
Monthly Cost = GPU Instance Cost × (1 / Utilization Rate)

Example:
- GPU Instance: $10,000/month
- Utilization: 40%
- Effective Cost: $10,000 / 0.40 = $25,000/month
- Waste: $15,000/month (60% idle capacity)
----

== Mitigating GPU Underutilization

=== Strategy 1: Model Quantization

**Action**: Quantize model to reduce memory footprint

**Expected Results:**
- 50% memory reduction (INT8)
- 2x more concurrent requests
- 85-90% GPU utilization

**Implementation**: See Module 2

=== Strategy 2: Optimized Serving (vLLM)

**Action**: Deploy vLLM with optimized configuration

**Key Settings:**
- `--gpu-memory-utilization=0.9`: Use 90% of GPU memory
- `--kv-cache-dtype=fp8`: Efficient KV cache
- Continuous batching: Enabled by default

**Expected Results:**
- 2-4x throughput improvement
- 85-90% GPU utilization
- Better memory efficiency

**Implementation**: See Module 3

=== Strategy 3: Distributed Inference (llm-d)

**Action**: Deploy llm-d for disaggregated inference

**Benefits:**
- Independent scaling of prefill/decode
- Better resource utilization
- Higher throughput for high-concurrency workloads

**Implementation**: See Module 3

== Real-World Impact

=== Before Optimization

**Configuration:**
- Model: Llama 3.1 8B FP16
- Serving: Standard runtime
- GPU: NVIDIA A10G (24GB)

**Metrics:**
- GPU Utilization: 38%
- Throughput: 5.2 RPS
- Cost per Request: $0.10
- Monthly Cost (10K requests): $1,000

=== After Optimization

**Configuration:**
- Model: Llama 3.1 8B INT8 (quantized)
- Serving: vLLM optimized
- GPU: NVIDIA A10G (24GB)

**Metrics:**
- GPU Utilization: 87%
- Throughput: 18.5 RPS (3.6x improvement)
- Cost per Request: $0.05 (50% reduction)
- Monthly Cost (10K requests): $500

**Savings:**
- 50% cost reduction
- 3.6x throughput improvement
- 2.3x better GPU utilization

== Key Takeaways

- GPU underutilization (<40%) is the primary cost driver
- Memory management (especially KV cache) is the bottleneck
- Optimization can achieve 85-90% utilization
- 2-4x throughput improvement is achievable
- 50%+ cost reduction is realistic

== Next Steps

- Review your current GPU utilization
- Identify memory bottlenecks
- Proceed to Module 2 for model optimization techniques

[NOTE]
====
**Documentation Gaps**: This section would benefit from:
- Screenshots of nvidia-smi showing <40% vs. 85-90% utilization
- Visual diagram of memory fragmentation
- GPU utilization timeline charts
- Cost calculator spreadsheet
====