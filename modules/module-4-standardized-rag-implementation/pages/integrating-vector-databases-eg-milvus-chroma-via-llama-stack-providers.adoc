= Integrating Vector Databases via Llama Stack Providers

== Introduction

For "Lean RAG" deployments, RHOAI 3.0's Llama Stack supports **embedded vector stores** that run entirely within the LlamaStackDistribution instance, eliminating external database dependencies. For production-scale deployments, external vector databases like Milvus and Chroma are also supported.

**Key Decision**: Choose embedded stores for development and rapid iteration, external stores for production scale.

== Embedded Vector Stores for "Lean RAG"

=== Inline FAISS (Technology Preview)

**Inline FAISS** is a lightweight vector store that runs entirely within the `LlamaStackDistribution` instance using an embedded SQLite backend.

**Characteristics:**
- **No External Dependencies**: Runs embedded in the pod
- **SQLite Backend**: Lightweight, file-based storage
- **Zero Configuration**: No separate database service needed
- **Perfect for Development**: Rapid iteration and testing

**Configuration:**

[source,yaml]
----
spec:
  vectorStore:
    provider: inline-faiss
    config:
      collectionName: lean-rag-documents
      embeddingModel: hf://sentence-transformers/all-MiniLM-L6-v2
      dimension: 384
      # Uses embedded SQLite backend
      # No external service required
----

**Use Cases:**
- Local experimentation
- Disconnected environments
- Single-node RAG deployments
- Rapid prototyping
- Development and testing

**Limitations:**
- Technology Preview (may have limitations)
- Best for small to medium datasets (<100K documents)
- Not suitable for high-scale production
- Single-node only

=== Inline Milvus Lite

**Inline Milvus Lite** runs embedded within the `LlamaStackDistribution` pod, using a local SQLite database for vector storage.

**Characteristics:**
- **Embedded Deployment**: Runs within the pod
- **Local SQLite**: File-based storage
- **Lightweight**: Minimal resource overhead
- **Easy Setup**: No external infrastructure

**Configuration:**

[source,yaml]
----
spec:
  vectorStore:
    provider: inline-milvus-lite
    config:
      collectionName: lean-rag-documents
      embeddingModel: hf://sentence-transformers/all-MiniLM-L6-v2
      dimension: 384
      # Runs embedded within LlamaStackDistribution pod
      # Uses local SQLite database
----

**Use Cases:**
- Lightweight experimentation
- Small-scale development
- Single-node deployments
- Testing and validation

**When to Use:**
- Development environments
- Rapid prototyping
- Small datasets (<50K documents)
- Resource-constrained environments

== External Vector Databases for Production

=== Milvus

**Milvus** is an open-source vector database designed for production-scale deployments.

**Characteristics:**
- **Scalable**: Handles millions of vectors
- **High Performance**: Optimized for similarity search
- **Production Ready**: Enterprise-grade reliability
- **Flexible**: Supports various index types and metrics

**Configuration:**

[source,yaml]
----
spec:
  vectorStore:
    provider: milvus
    config:
      collectionName: lean-rag-documents
      connection:
        host: milvus-service.lean-rag-accelerator.svc.cluster.local
        port: 19530
        database: default
      embeddingModel: hf://sentence-transformers/all-MiniLM-L6-v2
      dimension: 384
      indexType: HNSW
      metricType: L2
----

**When to Use:**
- Production deployments
- Large-scale datasets (>100K documents)
- High-throughput requirements
- Multi-node deployments
- Enterprise use cases

=== Chroma

**Chroma** is a vector database built for speed and scalability.

**Characteristics:**
- **Fast**: Optimized for speed
- **Scalable**: Handles large-scale deployments
- **Simple**: Easy to use and configure
- **Flexible**: Supports various embedding models

**Configuration:**

[source,yaml]
----
spec:
  vectorStore:
    provider: chroma
    config:
      collectionName: lean-rag-documents
      connection:
        host: chroma-service.lean-rag-accelerator.svc.cluster.local
        port: 8000
      embeddingModel: hf://sentence-transformers/all-MiniLM-L6-v2
      dimension: 384
----

**When to Use:**
- Production deployments
- Medium to large-scale datasets
- When simplicity is important
- Alternative to Milvus

== Decision Framework: Embedded vs. External

=== Choose Embedded (Inline FAISS or Inline Milvus Lite) When:

- ✅ Development and testing
- ✅ Rapid prototyping
- ✅ Small to medium datasets (<100K documents)
- ✅ Single-node deployments
- ✅ Minimal infrastructure overhead
- ✅ Disconnected environments
- ✅ Cost-sensitive deployments

=== Choose External (Milvus or Chroma) When:

- ✅ Production deployments
- ✅ Large-scale datasets (>100K documents)
- ✅ High-throughput requirements
- ✅ Multi-node deployments
- ✅ Enterprise use cases
- ✅ Need for horizontal scaling
- ✅ High availability requirements

== Configuration Examples

=== Complete LLMInferenceService with Inline FAISS

[source,yaml]
----
apiVersion: llama.redhat.com/v1alpha1
kind: LLMInferenceService
metadata:
  name: lean-rag-inference
  namespace: lean-rag-accelerator
spec:
  model:
    source: hf://meta-llama/Llama-3.1-8B-Instruct
  
  # Embedded vector store for Lean RAG
  vectorStore:
    provider: inline-faiss
    config:
      collectionName: lean-rag-documents
      embeddingModel: hf://sentence-transformers/all-MiniLM-L6-v2
      dimension: 384
  
  inference:
    runtime: vllm
    resources:
      limits:
        nvidia.com/gpu: 1
  
  rag:
    retrieval:
      strategy: semantic_search
      topK: 5
----

=== Complete LLMInferenceService with External Milvus

[source,yaml]
----
apiVersion: llama.redhat.com/v1alpha1
kind: LLMInferenceService
metadata:
  name: lean-rag-inference-prod
  namespace: lean-rag-accelerator
spec:
  model:
    source: oci://registry.example.com/models/llama-3.1-8b-int8:v1.0
  
  # External Milvus for production
  vectorStore:
    provider: milvus
    config:
      collectionName: lean-rag-documents
      connection:
        host: milvus-service.lean-rag-accelerator.svc.cluster.local
        port: 19530
      embeddingModel: hf://sentence-transformers/all-MiniLM-L6-v2
      dimension: 384
      indexType: HNSW
      metricType: L2
  
  inference:
    runtime: vllm
    resources:
      limits:
        nvidia.com/gpu: 1
  
  rag:
    retrieval:
      strategy: semantic_search
      topK: 5
----

== Performance Considerations

=== Embedded Stores

**Advantages:**
- Low latency (no network overhead)
- Simple deployment
- Cost effective
- Fast for small datasets

**Limitations:**
- Limited scalability
- Single-node only
- Memory constraints
- Not suitable for large datasets

=== External Stores

**Advantages:**
- High scalability
- Multi-node support
- Better for large datasets
- Production-grade reliability

**Limitations:**
- Network latency
- Additional infrastructure
- More complex deployment
- Higher operational overhead

== Migration Path

=== From Embedded to External

**When to Migrate:**
- Dataset grows beyond 100K documents
- Need for horizontal scaling
- Production deployment
- High availability requirements

**Migration Steps:**
1. Deploy external vector database (Milvus/Chroma)
2. Export data from embedded store
3. Import data to external store
4. Update LLMInferenceService configuration
5. Verify functionality
6. Decommission embedded store

== Best Practices

=== Embedded Stores

- **Use for Development**: Start with embedded stores
- **Monitor Resources**: Watch memory usage
- **Plan Migration**: Prepare for migration to external when scaling
- **Backup Data**: Export data regularly for embedded stores

=== External Stores

- **Deploy Separately**: Use dedicated infrastructure
- **Configure Indexing**: Optimize index types for your use case
- **Monitor Performance**: Track query latency and throughput
- **Plan Scaling**: Design for horizontal scaling

== Troubleshooting

=== Embedded Store Issues

**Problem**: Out of memory

**Solutions:**
- Reduce dataset size
- Use smaller embedding dimensions
- Migrate to external store
- Increase pod memory limits

**Problem**: Slow queries

**Solutions:**
- Optimize chunk size
- Reduce number of documents
- Consider migrating to external store

=== External Store Issues

**Problem**: Connection errors

**Solutions:**
- Verify service is running
- Check network connectivity
- Verify connection configuration
- Check firewall rules

**Problem**: Slow queries

**Solutions:**
- Optimize index configuration
- Increase resources
- Check network latency
- Review query patterns

== Key Takeaways

- **Embedded stores** (Inline FAISS, Inline Milvus Lite) are ideal for "Lean RAG"
- **External stores** (Milvus, Chroma) are for production scale
- Choose embedded for development, external for production
- Embedded stores eliminate external dependencies
- Migration path exists from embedded to external
- Performance characteristics differ between options

== Next Steps

- Learn about RAG evaluation with RAGAS (Section 4.4)
- Understand GitOps deployment (Section 4.5)
- Review complete RAG deployment examples

[NOTE]
====
**Documentation Gaps**: This section would benefit from:
- Performance benchmarks (embedded vs. external)
- Migration guide with examples
- Capacity planning guidelines
- Troubleshooting flowcharts
- Real-world use case examples
====