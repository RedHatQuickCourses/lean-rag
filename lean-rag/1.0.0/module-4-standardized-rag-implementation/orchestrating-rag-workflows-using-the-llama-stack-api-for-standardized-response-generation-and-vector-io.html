<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Orchestrating RAG Workflows with the Llama Stack API :: Lean RAG Accelerator QuickStart Training</title>
    <link rel="prev" href="ingesting-and-chunking-unstructured-enterprise-data-pdfs-using-docling.html">
    <link rel="next" href="integrating-vector-databases-eg-milvus-chroma-via-llama-stack-providers.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">Lean RAG Accelerator QuickStart Training</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="lean-rag" data-version="1.0.0">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Lean RAG Accelerator QuickStart Training</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Home</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../LABENV/index.html">Lab Environment</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../module-1-inference-economics-rhoai-30-architecture/module-1-inference-economics-rhoai-30-architecture.html">Module 1: Inference Economics &amp; RHOAI 3.0 Architecture</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-1-inference-economics-rhoai-30-architecture/analysis-of-the-impossible-trinity-in-llm-deployments-balancing-accuracy-latency-and-cost.html">Analysis of the "Impossible Trinity" in LLM Deployments: Balancing Accuracy, Latency, and Cost</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-1-inference-economics-rhoai-30-architecture/identifying-critical-gpu-underutilization-and-the-role-of-memory-management-in-inference-costs.html">Identifying "Critical GPU Underutilization" and the Role of Memory Management in Inference Costs</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-1-inference-economics-rhoai-30-architecture/overview-of-the-red-hat-ai-30-ai-factory-approach-for-industrialized-model-serving.html">Overview of the Red Hat OpenShift AI 3.0 "AI Factory" Approach for Industrialized Model Serving</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-1-inference-economics-rhoai-30-architecture/mapping-the-rhoai-30-inference-stack-vllm-llm-d-and-kserve-integration.html">Mapping the RHOAI 3.0 inference stack: vLLM, llm-d, and KServe integration</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../module-2-model-optimization-with-llm-compressor/module-2-model-optimization-with-llm-compressor.html">Module 2: Model Optimization with LLM Compressor</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-2-model-optimization-with-llm-compressor/techniques-for-model-compression-quantization-fp8-int8-and-sparsity.html">Techniques for Model Compression: Quantization (FP8, INT8) and Sparsity</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-2-model-optimization-with-llm-compressor/using-llm-compressor-to-create-optimized-modelcars-containerized-models.html">Using LLM Compressor to Create Optimized Models</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-2-model-optimization-with-llm-compressor/implementing-quantization-recipes-eg-smoothquant-gptq-to-reduce-gpu-memory-footprint.html">Implementing quantization recipes (e.g., SmoothQuant, GPTQ) to reduce GPU memory footprint</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-2-model-optimization-with-llm-compressor/accessing-validated-pre-optimized-models-eg-lean-llama-from-the-red-hat-model-catalog.html">Accessing Validated, Pre-optimized Models from the Red Hat Model Catalog</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../module-3-high-performance-serving-with-vllm-llm-d/module-3-high-performance-serving-with-vllm-llm-d.html">Module 3: High-Performance Serving with vLLM &amp; llm-d</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-3-high-performance-serving-with-vllm-llm-d/configuring-vllm-for-maximum-throughput-using-pagedattention-and-continuous-batching.html">Configuring vLLM for Maximum Throughput Using PagedAttention and Continuous Batching</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-3-high-performance-serving-with-vllm-llm-d/tuning-vllm-engine-arguments---gpu-memory-utilization-and---max-num-seqs.html">tuning vLLM engine arguments: <code>--gpu-memory-utilization</code> and <code>--max-num-seqs</code></a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-3-high-performance-serving-with-vllm-llm-d/deploying-distributed-inference-with-llm-d-to-separate-prefill-and-decode-phases-disaggregation.html">Deploying distributed inference with llm-d to separate prefill and decode phases (disaggregation)</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-3-high-performance-serving-with-vllm-llm-d/implementing-prefix-caching-to-reuse-key-value-kv-cache-for-rag-system-prompts.html">Implementing prefix caching to reuse Key-Value (KV) cache for RAG system prompts</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="module-4-standardized-rag-implementation.html">Module 4: Standardized RAG Implementation</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="ingesting-and-chunking-unstructured-enterprise-data-pdfs-using-docling.html">Ingesting and Chunking Unstructured Enterprise Data using Docling</a>
  </li>
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="orchestrating-rag-workflows-using-the-llama-stack-api-for-standardized-response-generation-and-vector-io.html">Orchestrating RAG Workflows with the Llama Stack API</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="integrating-vector-databases-eg-milvus-chroma-via-llama-stack-providers.html">Integrating Vector Databases via Llama Stack Providers</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="deploying-the-rag-application-stack-using-gitops-and-argocd-for-repeatability.html">deploying the RAG application stack using GitOps and ArgoCD for repeatability</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../module-5-validation-benchmarking/module-5-validation-benchmarking.html">Module 5: Validation &amp; Benchmarking</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-5-validation-benchmarking/defining-service-level-objectives-slos-for-time-to-first-token-ttft-and-inter-token-latency-itl.html">Defining Service Level Objectives (SLOs) for Time To First Token (TTFT) and Inter-Token Latency (ITL)</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-5-validation-benchmarking/simulating-real-world-traffic-load-using-guidellm-to-validate-throughput-gains.html">Simulating real-world traffic load using GuideLLM to validate throughput gains</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-5-validation-benchmarking/comparing-performance-metrics-throughput-vs-latency.html">Comparing performance metrics (throughput vs. latency</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../appendix/appendix.html">Appendix</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Lean RAG Accelerator QuickStart Training</span>
    <span class="version">1.0.0</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Lean RAG Accelerator QuickStart Training</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1.0.0</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Lean RAG Accelerator QuickStart Training</a></li>
    <li><a href="module-4-standardized-rag-implementation.html">Module 4: Standardized RAG Implementation</a></li>
    <li><a href="orchestrating-rag-workflows-using-the-llama-stack-api-for-standardized-response-generation-and-vector-io.html">Orchestrating RAG Workflows with the Llama Stack API</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Orchestrating RAG Workflows with the Llama Stack API</h1>
<div class="sect1">
<h2 id="_introduction"><a class="anchor" href="#_introduction"></a>Introduction</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The <strong>Llama Stack Operator</strong> in RHOAI 3.0 provides a unified, standardized approach to RAG workflows. This section covers how to orchestrate RAG workflows using the <strong>LLMInferenceService</strong> Custom Resource, which integrates LLM inference, semantic retrieval, and vector database operations.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_understanding_llama_stack_architecture"><a class="anchor" href="#_understanding_llama_stack_architecture"></a>Understanding Llama Stack Architecture</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_llama_stack_components"><a class="anchor" href="#_llama_stack_components"></a>Llama Stack Components</h3>
<div class="paragraph">
<p>The Llama Stack integrates three key capabilities:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>LLM Inference</strong>: High-performance model serving (vLLM/llm-d)</p>
</li>
<li>
<p><strong>Semantic Retrieval</strong>: Vector search and retrieval operations</p>
</li>
<li>
<p><strong>Vector Database</strong>: Embedded or external vector storage</p>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="_llminferenceservice_custom_resource"><a class="anchor" href="#_llminferenceservice_custom_resource"></a>LLMInferenceService Custom Resource</h3>
<div class="paragraph">
<p>The <strong>LLMInferenceService</strong> CR is the primary interface for deploying RAG systems with Llama Stack. It replaces the need for separate inference services, vector stores, and orchestration components.</p>
</div>
<div class="paragraph">
<p><strong>Key Features:</strong>
- Unified configuration for RAG workflows
- Embedded vector store support
- Standardized API endpoints
- Integration with RHOAI 3.0 infrastructure</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_deploying_rag_with_llminferenceservice"><a class="anchor" href="#_deploying_rag_with_llminferenceservice"></a>Deploying RAG with LLMInferenceService</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_basic_llminferenceservice_configuration"><a class="anchor" href="#_basic_llminferenceservice_configuration"></a>Basic LLMInferenceService Configuration</h3>
<div class="paragraph">
<p>The <code>LLMInferenceService</code> CRD structure includes the following key fields:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: llama.redhat.com/v1alpha1  # Verify exact API version
kind: LLMInferenceService
metadata:
  name: lean-rag-inference  # Unique name for the inference service
  namespace: lean-rag-accelerator
spec:
  replicas: 1  # Desired number of replicas

  # Model Configuration
  model:
    # Model URI - supports multiple formats:
    # - HuggingFace: hf://model-name/optional-hash
    # - S3: s3://bucket-name/object-key
    # - OCI Container: oci://registry/model:tag
    uri: hf://meta-llama/Llama-3.1-8B-Instruct
    # Or use ModelCar from Red Hat Catalog:
    # uri: oci://registry.redhat.io/rhelai1/modelcar-granite-3-1-8b-instruct:1.5

    # Model name used in chat completion requests
    name: llama-3.1-8b

  # Resource Configuration
  template:
    containers:
    - name: main
      resources:
        requests:
          cpu: "2"
          memory: 16Gi
          nvidia.com/gpu: "1"
        limits:
          cpu: "4"
          memory: 32Gi
          nvidia.com/gpu: "1"

  # Vector Store Configuration (Embedded for Lean RAG)
  vectorStore:
    provider: inline-faiss  # Options: inline-faiss, inline-milvus-lite, milvus, chroma
    config:
      collectionName: lean-rag-documents
      embeddingModel: hf://sentence-transformers/all-MiniLM-L6-v2
      dimension: 384  # Dimension for all-MiniLM-L6-v2

  # RAG Workflow Configuration
  rag:
    retrieval:
      strategy: semantic_search
      topK: 5
      scoreThreshold: 0.7
    generation:
      temperature: 0.7
      maxTokens: 512
      topP: 0.9</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p><strong>CRD Structure</strong>: The exact LLMInferenceService CRD schema may vary. The structure above is based on the llm-d LLMInferenceService pattern. Verify the exact schema for Llama Stack&#8217;s LLMInferenceService in RHOAI 3.0 documentation.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p><strong>Documentation Gap</strong>: Need to verify:
- Exact API version for LLMInferenceService CR
- Available provider options for vectorStore
- Configuration schema details
- Model source URI formats</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_model_source_options"><a class="anchor" href="#_model_source_options"></a>Model Source Options</h3>
<div class="paragraph">
<p><strong>1. HuggingFace URI:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  model:
    source: hf://meta-llama/Llama-3.1-8B-Instruct
    # Optional: specify hash for versioning
    # source: hf://meta-llama/Llama-3.1-8B-Instruct/abc123def456</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>2. S3 URI:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  model:
    source: s3://my-model-bucket/models/llama-3.1-8b-int8
    # S3 credentials configured via Secret</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>3. OCI Container (ModelCar):</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  model:
    source: oci://registry.example.com/models/llama-3.1-8b-int8:v1.0
    # ModelCar images reduce startup time and disk usage</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p><strong>ModelCars</strong>: OCI container images containing pre-packaged models. In KServe, deploying from OCI containers is known as "modelcars". This method:
- Reduces startup times (pre-fetched images)
- Reduces disk usage
- Enables versioning and sharing
- RHEL AI uses OCI artifact images
- OpenShift AI may use ModelCar images</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_embedded_vector_store_configuration"><a class="anchor" href="#_embedded_vector_store_configuration"></a>Embedded Vector Store Configuration</h3>
<div class="paragraph">
<p><strong>For "Lean RAG" deployments, use embedded vector stores:</strong></p>
</div>
<div class="sect3">
<h4 id="_inline_faiss_technology_preview"><a class="anchor" href="#_inline_faiss_technology_preview"></a>Inline FAISS (Technology Preview)</h4>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  vectorStore:
    provider: inline-faiss
    config:
      collectionName: lean-rag-documents
      embeddingModel: hf://sentence-transformers/all-MiniLM-L6-v2
      dimension: 384
      # Uses embedded SQLite backend
      # No external database service required</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Advantages:</strong>
- No external dependencies
- Lightweight and fast
- Perfect for development and rapid iteration
- Suitable for disconnected environments
- Single-node deployments</p>
</div>
<div class="paragraph">
<p><strong>Limitations:</strong>
- Technology Preview (may have limitations)
- Best for small to medium datasets
- Not suitable for high-scale production (use external Milvus)</p>
</div>
</div>
<div class="sect3">
<h4 id="_inline_milvus_lite"><a class="anchor" href="#_inline_milvus_lite"></a>Inline Milvus Lite</h4>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  vectorStore:
    provider: inline-milvus-lite
    config:
      collectionName: lean-rag-documents
      embeddingModel: hf://sentence-transformers/all-MiniLM-L6-v2
      dimension: 384
      # Runs embedded within LlamaStackDistribution pod
      # Uses local SQLite database</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Advantages:</strong>
- Embedded within the pod
- Local SQLite backend
- Good for lightweight experimentation
- Small-scale development</p>
</div>
<div class="paragraph">
<p><strong>When to Use:</strong>
- Development and testing
- Rapid prototyping
- Small datasets (&lt;100K documents)
- Single-node deployments</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_external_vector_store_configuration"><a class="anchor" href="#_external_vector_store_configuration"></a>External Vector Store Configuration</h3>
<div class="paragraph">
<p><strong>For production deployments, use external vector stores:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  vectorStore:
    provider: milvus  # or chroma
    config:
      collectionName: lean-rag-documents
      connection:
        host: milvus-service.lean-rag-accelerator.svc.cluster.local
        port: 19530
      embeddingModel: hf://sentence-transformers/all-MiniLM-L6-v2
      dimension: 384</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_rag_workflow_configuration"><a class="anchor" href="#_rag_workflow_configuration"></a>RAG Workflow Configuration</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_retrieval_configuration"><a class="anchor" href="#_retrieval_configuration"></a>Retrieval Configuration</h3>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  rag:
    retrieval:
      strategy: semantic_search  # Options: semantic_search, hybrid, keyword
      topK: 5  # Number of documents to retrieve
      rerank: false  # Enable reranking (optional)
      scoreThreshold: 0.7  # Minimum similarity score</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_response_generation_configuration"><a class="anchor" href="#_response_generation_configuration"></a>Response Generation Configuration</h3>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  rag:
    generation:
      temperature: 0.7  # Creativity (0.0-1.0)
      maxTokens: 512  # Maximum response length
      topP: 0.9  # Nucleus sampling
      topK: 40  # Top-k sampling
      stopSequences: []  # Stop generation on these sequences</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_complete_example_lean_rag_deployment"><a class="anchor" href="#_complete_example_lean_rag_deployment"></a>Complete Example: Lean RAG Deployment</h2>
<div class="sectionbody">
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: llama.redhat.com/v1alpha1
kind: LLMInferenceService
metadata:
  name: lean-rag-inference
  namespace: lean-rag-accelerator
spec:
  # Use quantized model from Module 2
  model:
    source: pvc://model-storage-pvc/models/llama-3.1-8b-int8
    # Or use ModelCar:
    # source: oci://registry.example.com/models/llama-3.1-8b-int8:v1.0

  # Embedded vector store for Lean RAG
  vectorStore:
    provider: inline-faiss
    config:
      collectionName: lean-rag-documents
      embeddingModel: hf://sentence-transformers/all-MiniLM-L6-v2
      dimension: 384

  # Inference configuration
  inference:
    runtime: vllm
    resources:
      limits:
        nvidia.com/gpu: 1
      requests:
        nvidia.com/gpu: 1

  # RAG workflow
  rag:
    retrieval:
      strategy: semantic_search
      topK: 5
      scoreThreshold: 0.7
    generation:
      temperature: 0.7
      maxTokens: 512
      topP: 0.9

  # API configuration
  api:
    port: 8080
    endpoints:
      - /query
      - /ingest
      - /health</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_deploying_llminferenceservice"><a class="anchor" href="#_deploying_llminferenceservice"></a>Deploying LLMInferenceService</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_step_1_apply_the_configuration"><a class="anchor" href="#_step_1_apply_the_configuration"></a>Step 1: Apply the Configuration</h3>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">kubectl apply -f llm-inferenceservice.yaml</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_step_2_monitor_deployment"><a class="anchor" href="#_step_2_monitor_deployment"></a>Step 2: Monitor Deployment</h3>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Watch service status
kubectl get llminferenceservice lean-rag-inference -w

# Check pods
kubectl get pods -l app=lean-rag-inference

# Check LlamaStackDistribution
kubectl get llamastackdistribution -n lean-rag-accelerator</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_step_3_verify_service"><a class="anchor" href="#_step_3_verify_service"></a>Step 3: Verify Service</h3>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Get service endpoint
SERVICE_URL=$(kubectl get llminferenceservice lean-rag-inference -o jsonpath='{.status.url}')

# Test health endpoint
curl $SERVICE_URL/health

# Test query endpoint
curl -X POST $SERVICE_URL/query \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What is the main topic?",
    "top_k": 5
  }'</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_standardized_api_endpoints"><a class="anchor" href="#_standardized_api_endpoints"></a>Standardized API Endpoints</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The Llama Stack provides comprehensive APIs, including internal APIs and OpenAI-compatible APIs, generally served through the distribution instance on <strong>port 8321</strong>.</p>
</div>
<div class="sect2">
<h3 id="_openai_compatible_apis"><a class="anchor" href="#_openai_compatible_apis"></a>OpenAI-Compatible APIs</h3>
<div class="sect3">
<h4 id="_chat_completions_technology_preview"><a class="anchor" href="#_chat_completions_technology_preview"></a>Chat Completions (Technology Preview)</h4>
<div class="paragraph">
<p><strong>Endpoint</strong>: <code>POST /v1/openai/v1/chat/completions</code></p>
</div>
<div class="paragraph">
<p><strong>Status</strong>: Technology Preview</p>
</div>
<div class="paragraph">
<p><strong>Request:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-json hljs" data-lang="json">{
  "model": "llama-3.1-8b",
  "messages": [
    {"role": "user", "content": "Your question here"}
  ],
  "temperature": 0.7,
  "max_tokens": 512
}</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_text_completions_technology_preview"><a class="anchor" href="#_text_completions_technology_preview"></a>Text Completions (Technology Preview)</h4>
<div class="paragraph">
<p><strong>Endpoint</strong>: <code>POST /v1/openai/v1/completions</code></p>
</div>
<div class="paragraph">
<p><strong>Status</strong>: Technology Preview</p>
</div>
<div class="paragraph">
<p><strong>Request:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-json hljs" data-lang="json">{
  "model": "llama-3.1-8b",
  "prompt": "Your prompt here",
  "temperature": 0.7,
  "max_tokens": 512
}</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_embeddings_supported"><a class="anchor" href="#_embeddings_supported"></a>Embeddings (Supported)</h4>
<div class="paragraph">
<p><strong>Endpoint</strong>: <code>POST /v1/openai/v1/embeddings</code></p>
</div>
<div class="paragraph">
<p><strong>Status</strong>: Supported</p>
</div>
<div class="paragraph">
<p><strong>Request:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-json hljs" data-lang="json">{
  "model": "granite-embedding-125m",
  "input": "Text to embed"
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_rag_and_vector_store_apis_technology_preview"><a class="anchor" href="#_rag_and_vector_store_apis_technology_preview"></a>RAG and Vector Store APIs (Technology Preview)</h3>
<div class="sect3">
<h4 id="_files_api"><a class="anchor" href="#_files_api"></a>Files API</h4>
<div class="paragraph">
<p><strong>Endpoint</strong>: <code>POST /v1/openai/v1/files</code></p>
</div>
<div class="paragraph">
<p><strong>Status</strong>: Technology Preview</p>
</div>
<div class="paragraph">
<p><strong>Purpose</strong>: Upload and manage files for RAG</p>
</div>
</div>
<div class="sect3">
<h4 id="_vector_stores_api"><a class="anchor" href="#_vector_stores_api"></a>Vector Stores API</h4>
<div class="paragraph">
<p><strong>Endpoint</strong>: <code>POST /v1/openai/v1/vector_stores/</code></p>
</div>
<div class="paragraph">
<p><strong>Status</strong>: Technology Preview</p>
</div>
<div class="paragraph">
<p><strong>Purpose</strong>: Create and manage vector stores</p>
</div>
</div>
<div class="sect3">
<h4 id="_vector_store_files"><a class="anchor" href="#_vector_store_files"></a>Vector Store Files</h4>
<div class="paragraph">
<p><strong>Endpoint</strong>: <code>POST /v1/openai/v1/vector_stores/{vector_store_id}/files</code></p>
</div>
<div class="paragraph">
<p><strong>Status</strong>: Developer Preview</p>
</div>
<div class="paragraph">
<p><strong>Purpose</strong>: Add files to a vector store</p>
</div>
</div>
<div class="sect3">
<h4 id="_responses_api"><a class="anchor" href="#_responses_api"></a>Responses API</h4>
<div class="paragraph">
<p><strong>Endpoint</strong>: <code>POST /v1/openai/v1/responses</code></p>
</div>
<div class="paragraph">
<p><strong>Status</strong>: Developer Preview (Experimental)</p>
</div>
<div class="paragraph">
<p><strong>Purpose</strong>: Generate RAG responses</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_llama_stack_core_apis"><a class="anchor" href="#_llama_stack_core_apis"></a>Llama Stack Core APIs</h3>
<div class="sect3">
<h4 id="_safety_api_technology_preview"><a class="anchor" href="#_safety_api_technology_preview"></a>Safety API (Technology Preview)</h4>
<div class="paragraph">
<p><strong>Endpoint</strong>: <code>POST /v1/safety</code></p>
</div>
<div class="paragraph">
<p><strong>Status</strong>: Technology Preview</p>
</div>
<div class="paragraph">
<p><strong>Purpose</strong>: Content safety and moderation</p>
</div>
</div>
<div class="sect3">
<h4 id="_agents_api_developer_preview"><a class="anchor" href="#_agents_api_developer_preview"></a>Agents API (Developer Preview)</h4>
<div class="paragraph">
<p><strong>Endpoint</strong>: <code>POST /v1alpha/agents</code></p>
</div>
<div class="paragraph">
<p><strong>Status</strong>: Developer Preview</p>
</div>
<div class="paragraph">
<p><strong>Purpose</strong>: Agent orchestration</p>
</div>
</div>
<div class="sect3">
<h4 id="_vector_io_api_developer_preview"><a class="anchor" href="#_vector_io_api_developer_preview"></a>Vector IO API (Developer Preview)</h4>
<div class="paragraph">
<p><strong>Endpoint</strong>: <code>POST /v1/vector-io</code></p>
</div>
<div class="paragraph">
<p><strong>Status</strong>: Developer Preview</p>
</div>
<div class="paragraph">
<p><strong>Purpose</strong>: Vector operations</p>
</div>
</div>
<div class="sect3">
<h4 id="_evaluation_api_developer_preview"><a class="anchor" href="#_evaluation_api_developer_preview"></a>Evaluation API (Developer Preview)</h4>
<div class="paragraph">
<p><strong>Endpoint</strong>: <code>POST /v1beta/eval</code></p>
</div>
<div class="paragraph">
<p><strong>Status</strong>: Developer Preview</p>
</div>
<div class="paragraph">
<p><strong>Purpose</strong>: RAG evaluation (RAGAS integration)</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_model_metadata_api"><a class="anchor" href="#_model_metadata_api"></a>Model Metadata API</h3>
<div class="sect3">
<h4 id="_models_list_technology_preview"><a class="anchor" href="#_models_list_technology_preview"></a>Models List (Technology Preview)</h4>
<div class="paragraph">
<p><strong>Endpoint</strong>: <code>GET /v1/openai/v1/models</code></p>
</div>
<div class="paragraph">
<p><strong>Status</strong>: Technology Preview</p>
</div>
<div class="paragraph">
<p><strong>Purpose</strong>: List available models</p>
</div>
<div class="paragraph">
<p><strong>Response:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-json hljs" data-lang="json">{
  "data": [
    {
      "id": "llama-3.1-8b",
      "object": "model",
      "created": 1234567890,
      "owned_by": "redhat"
    }
  ]
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_complete_api_reference"><a class="anchor" href="#_complete_api_reference"></a>Complete API Reference</h3>
<div class="paragraph">
<p>| API Category | Endpoint | Support Status |
|-------------|----------|----------------|
| <strong>OpenAI - Chat/Text</strong> | <code>/v1/openai/v1/chat/completions</code> | Technology Preview |
| | <code>/v1/openai/v1/completions</code> | Technology Preview |
| | <code>/v1/openai/v1/embeddings</code> | Supported |
| <strong>OpenAI - RAG/Agents</strong> | <code>/v1/openai/v1/files</code> | Technology Preview |
| | <code>/v1/openai/v1/vector_stores/</code> | Technology Preview |
| | <code>/v1/openai/v1/vector_stores/{vector_store_id}/files</code> | Developer Preview |
| | <code>/v1/openai/v1/responses</code> | Developer Preview (Experimental) |
| <strong>Llama Stack Core</strong> | <code>/v1/safety</code> | Technology Preview |
| | <code>/v1alpha/agents</code> | Developer Preview |
| | <code>/v1/vector-io</code> | Developer Preview |
| | <code>/v1beta/eval</code> | Developer Preview |
| <strong>Model Metadata</strong> | <code>/v1/openai/v1/models</code> | Technology Preview |</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p><strong>Important</strong>: Most Llama Stack APIs are in Technology Preview or Developer Preview. Only the Embeddings API (<code>/v1/openai/v1/embeddings</code>) is fully supported. Use Technology Preview and Developer Preview APIs with caution in production environments.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_vector_io_operations"><a class="anchor" href="#_vector_io_operations"></a>Vector IO Operations</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The Llama Stack API provides standardized Vector IO operations:</p>
</div>
<div class="sect2">
<h3 id="_adding_documents"><a class="anchor" href="#_adding_documents"></a>Adding Documents</h3>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">curl -X POST $SERVICE_URL/ingest \
  -H "Content-Type: application/json" \
  -d '{
    "documents": [
      {
        "id": "doc1",
        "text": "Document content",
        "metadata": {"title": "Example"}
      }
    ]
  }'</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_querying_documents"><a class="anchor" href="#_querying_documents"></a>Querying Documents</h3>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">curl -X POST $SERVICE_URL/query \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Search query",
    "top_k": 5
  }'</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_deleting_documents"><a class="anchor" href="#_deleting_documents"></a>Deleting Documents</h3>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">curl -X DELETE $SERVICE_URL/documents/doc1</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_integration_with_docling"><a class="anchor" href="#_integration_with_docling"></a>Integration with Docling</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Docling-processed documents can be ingested directly:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Process document with Docling (see Section 4.1)
# Then ingest processed chunks
curl -X POST $SERVICE_URL/ingest \
  -H "Content-Type: application/json" \
  -d @docling-output.json</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_best_practices"><a class="anchor" href="#_best_practices"></a>Best Practices</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_model_source_selection"><a class="anchor" href="#_model_source_selection"></a>Model Source Selection</h3>
<div class="ulist">
<ul>
<li>
<p><strong>Development</strong>: Use HuggingFace URIs for quick iteration</p>
</li>
<li>
<p><strong>Production</strong>: Use ModelCars (OCI containers) for reliability</p>
</li>
<li>
<p><strong>Optimized Models</strong>: Use quantized models from Module 2</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_vector_store_selection"><a class="anchor" href="#_vector_store_selection"></a>Vector Store Selection</h3>
<div class="ulist">
<ul>
<li>
<p><strong>Lean RAG (Development)</strong>: Use Inline FAISS or Inline Milvus Lite</p>
</li>
<li>
<p><strong>Production (Small Scale)</strong>: Use Inline Milvus Lite</p>
</li>
<li>
<p><strong>Production (Large Scale)</strong>: Use external Milvus</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_resource_allocation"><a class="anchor" href="#_resource_allocation"></a>Resource Allocation</h3>
<div class="ulist">
<ul>
<li>
<p><strong>GPU</strong>: 1 GPU for inference (A10G or equivalent)</p>
</li>
<li>
<p><strong>Memory</strong>: 4-8GB for embedded vector stores</p>
</li>
<li>
<p><strong>CPU</strong>: 2-4 cores for processing</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_troubleshooting"><a class="anchor" href="#_troubleshooting"></a>Troubleshooting</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_service_not_ready"><a class="anchor" href="#_service_not_ready"></a>Service Not Ready</h3>
<div class="paragraph">
<p><strong>Symptoms:</strong>
- LLMInferenceService status shows "NotReady"
- Pods not starting</p>
</div>
<div class="paragraph">
<p><strong>Solutions:</strong>
- Check model source accessibility
- Verify GPU resources available
- Check vector store configuration
- Review pod logs: <code>kubectl logs -l app=lean-rag-inference</code></p>
</div>
</div>
<div class="sect2">
<h3 id="_vector_store_errors"><a class="anchor" href="#_vector_store_errors"></a>Vector Store Errors</h3>
<div class="paragraph">
<p><strong>Symptoms:</strong>
- Embedding generation fails
- Document ingestion fails</p>
</div>
<div class="paragraph">
<p><strong>Solutions:</strong>
- Verify embedding model is accessible
- Check vector store provider configuration
- For embedded stores, check pod resources
- Review vector store logs</p>
</div>
</div>
<div class="sect2">
<h3 id="_inference_errors"><a class="anchor" href="#_inference_errors"></a>Inference Errors</h3>
<div class="paragraph">
<p><strong>Symptoms:</strong>
- Query endpoint returns errors
- Generation fails</p>
</div>
<div class="paragraph">
<p><strong>Solutions:</strong>
- Verify inference service (vLLM) is running
- Check model is loaded correctly
- Verify GPU availability
- Review inference service logs</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_key_takeaways"><a class="anchor" href="#_key_takeaways"></a>Key Takeaways</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>LLMInferenceService CR provides unified RAG deployment</p>
</li>
<li>
<p>Embedded vector stores (Inline FAISS, Inline Milvus Lite) enable "Lean RAG"</p>
</li>
<li>
<p>ModelCars (OCI containers) provide reliable model deployment</p>
</li>
<li>
<p>Standardized API endpoints simplify integration</p>
</li>
<li>
<p>Llama Stack integrates inference, retrieval, and vector storage</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_next_steps"><a class="anchor" href="#_next_steps"></a>Next Steps</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Learn about document ingestion with Docling (Section 4.1)</p>
</li>
<li>
<p>Explore embedded vector store configuration (Section 4.3)</p>
</li>
<li>
<p>Understand RAG evaluation with RAGAS (Section 4.4)</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p><strong>Documentation Gaps</strong>: This section would benefit from:
- Exact LLMInferenceService CRD schema
- Complete API reference documentation
- ModelCar creation and publishing guide
- Vector store provider comparison table
- Integration examples with Docling</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="ingesting-and-chunking-unstructured-enterprise-data-pdfs-using-docling.html">Ingesting and Chunking Unstructured Enterprise Data using Docling</a></span>
  <span class="next"><a href="integrating-vector-databases-eg-milvus-chroma-via-llama-stack-providers.html">Integrating Vector Databases via Llama Stack Providers</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
