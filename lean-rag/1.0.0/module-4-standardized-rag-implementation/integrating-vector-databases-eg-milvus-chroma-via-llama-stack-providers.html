<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Integrating Vector Databases via Llama Stack Providers :: Lean RAG Accelerator QuickStart Training</title>
    <link rel="prev" href="orchestrating-rag-workflows-using-the-llama-stack-api-for-standardized-response-generation-and-vector-io.html">
    <link rel="next" href="deploying-the-rag-application-stack-using-gitops-and-argocd-for-repeatability.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">Lean RAG Accelerator QuickStart Training</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="lean-rag" data-version="1.0.0">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Lean RAG Accelerator QuickStart Training</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Home</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../LABENV/index.html">Lab Environment</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../module-1-inference-economics-rhoai-30-architecture/module-1-inference-economics-rhoai-30-architecture.html">Module 1: Inference Economics &amp; RHOAI 3.0 Architecture</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-1-inference-economics-rhoai-30-architecture/analysis-of-the-impossible-trinity-in-llm-deployments-balancing-accuracy-latency-and-cost.html">Analysis of the "Impossible Trinity" in LLM Deployments: Balancing Accuracy, Latency, and Cost</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-1-inference-economics-rhoai-30-architecture/identifying-critical-gpu-underutilization-and-the-role-of-memory-management-in-inference-costs.html">Identifying "Critical GPU Underutilization" and the Role of Memory Management in Inference Costs</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-1-inference-economics-rhoai-30-architecture/overview-of-the-red-hat-ai-30-ai-factory-approach-for-industrialized-model-serving.html">Overview of the Red Hat OpenShift AI 3.0 "AI Factory" Approach for Industrialized Model Serving</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-1-inference-economics-rhoai-30-architecture/mapping-the-rhoai-30-inference-stack-vllm-llm-d-and-kserve-integration.html">Mapping the RHOAI 3.0 inference stack: vLLM, llm-d, and KServe integration</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../module-2-model-optimization-with-llm-compressor/module-2-model-optimization-with-llm-compressor.html">Module 2: Model Optimization with LLM Compressor</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-2-model-optimization-with-llm-compressor/techniques-for-model-compression-quantization-fp8-int8-and-sparsity.html">Techniques for Model Compression: Quantization (FP8, INT8) and Sparsity</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-2-model-optimization-with-llm-compressor/using-llm-compressor-to-create-optimized-modelcars-containerized-models.html">Using LLM Compressor to Create Optimized Models</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-2-model-optimization-with-llm-compressor/implementing-quantization-recipes-eg-smoothquant-gptq-to-reduce-gpu-memory-footprint.html">Implementing quantization recipes (e.g., SmoothQuant, GPTQ) to reduce GPU memory footprint</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-2-model-optimization-with-llm-compressor/accessing-validated-pre-optimized-models-eg-lean-llama-from-the-red-hat-model-catalog.html">Accessing Validated, Pre-optimized Models from the Red Hat Model Catalog</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../module-3-high-performance-serving-with-vllm-llm-d/module-3-high-performance-serving-with-vllm-llm-d.html">Module 3: High-Performance Serving with vLLM &amp; llm-d</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-3-high-performance-serving-with-vllm-llm-d/configuring-vllm-for-maximum-throughput-using-pagedattention-and-continuous-batching.html">Configuring vLLM for Maximum Throughput Using PagedAttention and Continuous Batching</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-3-high-performance-serving-with-vllm-llm-d/tuning-vllm-engine-arguments---gpu-memory-utilization-and---max-num-seqs.html">tuning vLLM engine arguments: <code>--gpu-memory-utilization</code> and <code>--max-num-seqs</code></a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-3-high-performance-serving-with-vllm-llm-d/deploying-distributed-inference-with-llm-d-to-separate-prefill-and-decode-phases-disaggregation.html">Deploying distributed inference with llm-d to separate prefill and decode phases (disaggregation)</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-3-high-performance-serving-with-vllm-llm-d/implementing-prefix-caching-to-reuse-key-value-kv-cache-for-rag-system-prompts.html">Implementing prefix caching to reuse Key-Value (KV) cache for RAG system prompts</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="module-4-standardized-rag-implementation.html">Module 4: Standardized RAG Implementation</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="ingesting-and-chunking-unstructured-enterprise-data-pdfs-using-docling.html">Ingesting and Chunking Unstructured Enterprise Data using Docling</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="orchestrating-rag-workflows-using-the-llama-stack-api-for-standardized-response-generation-and-vector-io.html">Orchestrating RAG Workflows with the Llama Stack API</a>
  </li>
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="integrating-vector-databases-eg-milvus-chroma-via-llama-stack-providers.html">Integrating Vector Databases via Llama Stack Providers</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="deploying-the-rag-application-stack-using-gitops-and-argocd-for-repeatability.html">deploying the RAG application stack using GitOps and ArgoCD for repeatability</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../module-5-validation-benchmarking/module-5-validation-benchmarking.html">Module 5: Validation &amp; Benchmarking</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-5-validation-benchmarking/defining-service-level-objectives-slos-for-time-to-first-token-ttft-and-inter-token-latency-itl.html">Defining Service Level Objectives (SLOs) for Time To First Token (TTFT) and Inter-Token Latency (ITL)</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-5-validation-benchmarking/simulating-real-world-traffic-load-using-guidellm-to-validate-throughput-gains.html">Simulating real-world traffic load using GuideLLM to validate throughput gains</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-5-validation-benchmarking/comparing-performance-metrics-throughput-vs-latency.html">Comparing performance metrics (throughput vs. latency</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../appendix/appendix.html">Appendix</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Lean RAG Accelerator QuickStart Training</span>
    <span class="version">1.0.0</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Lean RAG Accelerator QuickStart Training</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1.0.0</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Lean RAG Accelerator QuickStart Training</a></li>
    <li><a href="module-4-standardized-rag-implementation.html">Module 4: Standardized RAG Implementation</a></li>
    <li><a href="integrating-vector-databases-eg-milvus-chroma-via-llama-stack-providers.html">Integrating Vector Databases via Llama Stack Providers</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Integrating Vector Databases via Llama Stack Providers</h1>
<div class="sect1">
<h2 id="_introduction"><a class="anchor" href="#_introduction"></a>Introduction</h2>
<div class="sectionbody">
<div class="paragraph">
<p>For "Lean RAG" deployments, RHOAI 3.0&#8217;s Llama Stack supports <strong>embedded vector stores</strong> that run entirely within the LlamaStackDistribution instance, eliminating external database dependencies. For production-scale deployments, external vector databases like Milvus and Chroma are also supported.</p>
</div>
<div class="paragraph">
<p><strong>Key Decision</strong>: Choose embedded stores for development and rapid iteration, external stores for production scale.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_embedded_vector_stores_for_lean_rag"><a class="anchor" href="#_embedded_vector_stores_for_lean_rag"></a>Embedded Vector Stores for "Lean RAG"</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_inline_faiss_technology_preview"><a class="anchor" href="#_inline_faiss_technology_preview"></a>Inline FAISS (Technology Preview)</h3>
<div class="paragraph">
<p><strong>Inline FAISS</strong> is a lightweight vector store that runs entirely within the <code>LlamaStackDistribution</code> instance using an embedded SQLite backend.</p>
</div>
<div class="paragraph">
<p><strong>Characteristics:</strong>
- <strong>No External Dependencies</strong>: Runs embedded in the pod
- <strong>SQLite Backend</strong>: Lightweight, file-based storage
- <strong>Zero Configuration</strong>: No separate database service needed
- <strong>Perfect for Development</strong>: Rapid iteration and testing</p>
</div>
<div class="paragraph">
<p><strong>Configuration:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  vectorStore:
    provider: inline-faiss
    config:
      collectionName: lean-rag-documents
      embeddingModel: hf://sentence-transformers/all-MiniLM-L6-v2
      dimension: 384
      # Uses embedded SQLite backend
      # No external service required</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Use Cases:</strong>
- Local experimentation
- Disconnected environments
- Single-node RAG deployments
- Rapid prototyping
- Development and testing</p>
</div>
<div class="paragraph">
<p><strong>Limitations:</strong>
- Technology Preview (may have limitations)
- Best for small to medium datasets (&lt;100K documents)
- Not suitable for high-scale production
- Single-node only</p>
</div>
</div>
<div class="sect2">
<h3 id="_inline_milvus_lite"><a class="anchor" href="#_inline_milvus_lite"></a>Inline Milvus Lite</h3>
<div class="paragraph">
<p><strong>Inline Milvus Lite</strong> runs embedded within the <code>LlamaStackDistribution</code> pod, using a local SQLite database for vector storage.</p>
</div>
<div class="paragraph">
<p><strong>Characteristics:</strong>
- <strong>Embedded Deployment</strong>: Runs within the pod
- <strong>Local SQLite</strong>: File-based storage
- <strong>Lightweight</strong>: Minimal resource overhead
- <strong>Easy Setup</strong>: No external infrastructure</p>
</div>
<div class="paragraph">
<p><strong>Configuration:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  vectorStore:
    provider: inline-milvus-lite
    config:
      collectionName: lean-rag-documents
      embeddingModel: hf://sentence-transformers/all-MiniLM-L6-v2
      dimension: 384
      # Runs embedded within LlamaStackDistribution pod
      # Uses local SQLite database</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Use Cases:</strong>
- Lightweight experimentation
- Small-scale development
- Single-node deployments
- Testing and validation</p>
</div>
<div class="paragraph">
<p><strong>When to Use:</strong>
- Development environments
- Rapid prototyping
- Small datasets (&lt;50K documents)
- Resource-constrained environments</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_external_vector_databases_for_production"><a class="anchor" href="#_external_vector_databases_for_production"></a>External Vector Databases for Production</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_milvus"><a class="anchor" href="#_milvus"></a>Milvus</h3>
<div class="paragraph">
<p><strong>Milvus</strong> is an open-source vector database designed for production-scale deployments.</p>
</div>
<div class="paragraph">
<p><strong>Characteristics:</strong>
- <strong>Scalable</strong>: Handles millions of vectors
- <strong>High Performance</strong>: Optimized for similarity search
- <strong>Production Ready</strong>: Enterprise-grade reliability
- <strong>Flexible</strong>: Supports various index types and metrics</p>
</div>
<div class="paragraph">
<p><strong>Configuration:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  vectorStore:
    provider: milvus
    config:
      collectionName: lean-rag-documents
      connection:
        host: milvus-service.lean-rag-accelerator.svc.cluster.local
        port: 19530
        database: default
      embeddingModel: hf://sentence-transformers/all-MiniLM-L6-v2
      dimension: 384
      indexType: HNSW
      metricType: L2</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>When to Use:</strong>
- Production deployments
- Large-scale datasets (&gt;100K documents)
- High-throughput requirements
- Multi-node deployments
- Enterprise use cases</p>
</div>
</div>
<div class="sect2">
<h3 id="_chroma"><a class="anchor" href="#_chroma"></a>Chroma</h3>
<div class="paragraph">
<p><strong>Chroma</strong> is a vector database built for speed and scalability.</p>
</div>
<div class="paragraph">
<p><strong>Characteristics:</strong>
- <strong>Fast</strong>: Optimized for speed
- <strong>Scalable</strong>: Handles large-scale deployments
- <strong>Simple</strong>: Easy to use and configure
- <strong>Flexible</strong>: Supports various embedding models</p>
</div>
<div class="paragraph">
<p><strong>Configuration:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  vectorStore:
    provider: chroma
    config:
      collectionName: lean-rag-documents
      connection:
        host: chroma-service.lean-rag-accelerator.svc.cluster.local
        port: 8000
      embeddingModel: hf://sentence-transformers/all-MiniLM-L6-v2
      dimension: 384</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>When to Use:</strong>
- Production deployments
- Medium to large-scale datasets
- When simplicity is important
- Alternative to Milvus</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_decision_framework_embedded_vs_external"><a class="anchor" href="#_decision_framework_embedded_vs_external"></a>Decision Framework: Embedded vs. External</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_choose_embedded_inline_faiss_or_inline_milvus_lite_when"><a class="anchor" href="#_choose_embedded_inline_faiss_or_inline_milvus_lite_when"></a>Choose Embedded (Inline FAISS or Inline Milvus Lite) When:</h3>
<div class="ulist">
<ul>
<li>
<p>✅ Development and testing</p>
</li>
<li>
<p>✅ Rapid prototyping</p>
</li>
<li>
<p>✅ Small to medium datasets (&lt;100K documents)</p>
</li>
<li>
<p>✅ Single-node deployments</p>
</li>
<li>
<p>✅ Minimal infrastructure overhead</p>
</li>
<li>
<p>✅ Disconnected environments</p>
</li>
<li>
<p>✅ Cost-sensitive deployments</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_choose_external_milvus_or_chroma_when"><a class="anchor" href="#_choose_external_milvus_or_chroma_when"></a>Choose External (Milvus or Chroma) When:</h3>
<div class="ulist">
<ul>
<li>
<p>✅ Production deployments</p>
</li>
<li>
<p>✅ Large-scale datasets (&gt;100K documents)</p>
</li>
<li>
<p>✅ High-throughput requirements</p>
</li>
<li>
<p>✅ Multi-node deployments</p>
</li>
<li>
<p>✅ Enterprise use cases</p>
</li>
<li>
<p>✅ Need for horizontal scaling</p>
</li>
<li>
<p>✅ High availability requirements</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_configuration_examples"><a class="anchor" href="#_configuration_examples"></a>Configuration Examples</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_complete_llminferenceservice_with_inline_faiss"><a class="anchor" href="#_complete_llminferenceservice_with_inline_faiss"></a>Complete LLMInferenceService with Inline FAISS</h3>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: llama.redhat.com/v1alpha1
kind: LLMInferenceService
metadata:
  name: lean-rag-inference
  namespace: lean-rag-accelerator
spec:
  model:
    source: hf://meta-llama/Llama-3.1-8B-Instruct

  # Embedded vector store for Lean RAG
  vectorStore:
    provider: inline-faiss
    config:
      collectionName: lean-rag-documents
      embeddingModel: hf://sentence-transformers/all-MiniLM-L6-v2
      dimension: 384

  inference:
    runtime: vllm
    resources:
      limits:
        nvidia.com/gpu: 1

  rag:
    retrieval:
      strategy: semantic_search
      topK: 5</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_complete_llminferenceservice_with_external_milvus"><a class="anchor" href="#_complete_llminferenceservice_with_external_milvus"></a>Complete LLMInferenceService with External Milvus</h3>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: llama.redhat.com/v1alpha1
kind: LLMInferenceService
metadata:
  name: lean-rag-inference-prod
  namespace: lean-rag-accelerator
spec:
  model:
    source: oci://registry.example.com/models/llama-3.1-8b-int8:v1.0

  # External Milvus for production
  vectorStore:
    provider: milvus
    config:
      collectionName: lean-rag-documents
      connection:
        host: milvus-service.lean-rag-accelerator.svc.cluster.local
        port: 19530
      embeddingModel: hf://sentence-transformers/all-MiniLM-L6-v2
      dimension: 384
      indexType: HNSW
      metricType: L2

  inference:
    runtime: vllm
    resources:
      limits:
        nvidia.com/gpu: 1

  rag:
    retrieval:
      strategy: semantic_search
      topK: 5</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_performance_considerations"><a class="anchor" href="#_performance_considerations"></a>Performance Considerations</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_embedded_stores"><a class="anchor" href="#_embedded_stores"></a>Embedded Stores</h3>
<div class="paragraph">
<p><strong>Advantages:</strong>
- Low latency (no network overhead)
- Simple deployment
- Cost effective
- Fast for small datasets</p>
</div>
<div class="paragraph">
<p><strong>Limitations:</strong>
- Limited scalability
- Single-node only
- Memory constraints
- Not suitable for large datasets</p>
</div>
</div>
<div class="sect2">
<h3 id="_external_stores"><a class="anchor" href="#_external_stores"></a>External Stores</h3>
<div class="paragraph">
<p><strong>Advantages:</strong>
- High scalability
- Multi-node support
- Better for large datasets
- Production-grade reliability</p>
</div>
<div class="paragraph">
<p><strong>Limitations:</strong>
- Network latency
- Additional infrastructure
- More complex deployment
- Higher operational overhead</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_migration_path"><a class="anchor" href="#_migration_path"></a>Migration Path</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_from_embedded_to_external"><a class="anchor" href="#_from_embedded_to_external"></a>From Embedded to External</h3>
<div class="paragraph">
<p><strong>When to Migrate:</strong>
- Dataset grows beyond 100K documents
- Need for horizontal scaling
- Production deployment
- High availability requirements</p>
</div>
<div class="paragraph">
<p><strong>Migration Steps:</strong>
1. Deploy external vector database (Milvus/Chroma)
2. Export data from embedded store
3. Import data to external store
4. Update LLMInferenceService configuration
5. Verify functionality
6. Decommission embedded store</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_best_practices"><a class="anchor" href="#_best_practices"></a>Best Practices</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_embedded_stores_2"><a class="anchor" href="#_embedded_stores_2"></a>Embedded Stores</h3>
<div class="ulist">
<ul>
<li>
<p><strong>Use for Development</strong>: Start with embedded stores</p>
</li>
<li>
<p><strong>Monitor Resources</strong>: Watch memory usage</p>
</li>
<li>
<p><strong>Plan Migration</strong>: Prepare for migration to external when scaling</p>
</li>
<li>
<p><strong>Backup Data</strong>: Export data regularly for embedded stores</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_external_stores_2"><a class="anchor" href="#_external_stores_2"></a>External Stores</h3>
<div class="ulist">
<ul>
<li>
<p><strong>Deploy Separately</strong>: Use dedicated infrastructure</p>
</li>
<li>
<p><strong>Configure Indexing</strong>: Optimize index types for your use case</p>
</li>
<li>
<p><strong>Monitor Performance</strong>: Track query latency and throughput</p>
</li>
<li>
<p><strong>Plan Scaling</strong>: Design for horizontal scaling</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_troubleshooting"><a class="anchor" href="#_troubleshooting"></a>Troubleshooting</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_embedded_store_issues"><a class="anchor" href="#_embedded_store_issues"></a>Embedded Store Issues</h3>
<div class="paragraph">
<p><strong>Problem</strong>: Out of memory</p>
</div>
<div class="paragraph">
<p><strong>Solutions:</strong>
- Reduce dataset size
- Use smaller embedding dimensions
- Migrate to external store
- Increase pod memory limits</p>
</div>
<div class="paragraph">
<p><strong>Problem</strong>: Slow queries</p>
</div>
<div class="paragraph">
<p><strong>Solutions:</strong>
- Optimize chunk size
- Reduce number of documents
- Consider migrating to external store</p>
</div>
</div>
<div class="sect2">
<h3 id="_external_store_issues"><a class="anchor" href="#_external_store_issues"></a>External Store Issues</h3>
<div class="paragraph">
<p><strong>Problem</strong>: Connection errors</p>
</div>
<div class="paragraph">
<p><strong>Solutions:</strong>
- Verify service is running
- Check network connectivity
- Verify connection configuration
- Check firewall rules</p>
</div>
<div class="paragraph">
<p><strong>Problem</strong>: Slow queries</p>
</div>
<div class="paragraph">
<p><strong>Solutions:</strong>
- Optimize index configuration
- Increase resources
- Check network latency
- Review query patterns</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_key_takeaways"><a class="anchor" href="#_key_takeaways"></a>Key Takeaways</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p><strong>Embedded stores</strong> (Inline FAISS, Inline Milvus Lite) are ideal for "Lean RAG"</p>
</li>
<li>
<p><strong>External stores</strong> (Milvus, Chroma) are for production scale</p>
</li>
<li>
<p>Choose embedded for development, external for production</p>
</li>
<li>
<p>Embedded stores eliminate external dependencies</p>
</li>
<li>
<p>Migration path exists from embedded to external</p>
</li>
<li>
<p>Performance characteristics differ between options</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_next_steps"><a class="anchor" href="#_next_steps"></a>Next Steps</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Learn about RAG evaluation with RAGAS (Section 4.4)</p>
</li>
<li>
<p>Understand GitOps deployment (Section 4.5)</p>
</li>
<li>
<p>Review complete RAG deployment examples</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p><strong>Documentation Gaps</strong>: This section would benefit from:
- Performance benchmarks (embedded vs. external)
- Migration guide with examples
- Capacity planning guidelines
- Troubleshooting flowcharts
- Real-world use case examples</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="orchestrating-rag-workflows-using-the-llama-stack-api-for-standardized-response-generation-and-vector-io.html">Orchestrating RAG Workflows with the Llama Stack API</a></span>
  <span class="next"><a href="deploying-the-rag-application-stack-using-gitops-and-argocd-for-repeatability.html">deploying the RAG application stack using GitOps and ArgoCD for repeatability</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
