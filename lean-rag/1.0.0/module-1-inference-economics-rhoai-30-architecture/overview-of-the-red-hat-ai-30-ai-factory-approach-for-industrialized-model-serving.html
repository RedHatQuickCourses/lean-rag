<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Overview of the Red Hat OpenShift AI 3.0 "AI Factory" Approach for Industrialized Model Serving :: Lean RAG Accelerator QuickStart Training</title>
    <link rel="prev" href="identifying-critical-gpu-underutilization-and-the-role-of-memory-management-in-inference-costs.html">
    <link rel="next" href="mapping-the-rhoai-30-inference-stack-vllm-llm-d-and-kserve-integration.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">Lean RAG Accelerator QuickStart Training</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="lean-rag" data-version="1.0.0">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Lean RAG Accelerator QuickStart Training</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Home</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../LABENV/index.html">Lab Environment</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="module-1-inference-economics-rhoai-30-architecture.html">Module 1: Inference Economics &amp; RHOAI 3.0 Architecture</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="analysis-of-the-impossible-trinity-in-llm-deployments-balancing-accuracy-latency-and-cost.html">Analysis of the "Impossible Trinity" in LLM Deployments: Balancing Accuracy, Latency, and Cost</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="identifying-critical-gpu-underutilization-and-the-role-of-memory-management-in-inference-costs.html">Identifying "Critical GPU Underutilization" and the Role of Memory Management in Inference Costs</a>
  </li>
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="overview-of-the-red-hat-ai-30-ai-factory-approach-for-industrialized-model-serving.html">Overview of the Red Hat OpenShift AI 3.0 "AI Factory" Approach for Industrialized Model Serving</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="mapping-the-rhoai-30-inference-stack-vllm-llm-d-and-kserve-integration.html">Mapping the RHOAI 3.0 inference stack: vLLM, llm-d, and KServe integration</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../module-2-model-optimization-with-llm-compressor/module-2-model-optimization-with-llm-compressor.html">Module 2: Model Optimization with LLM Compressor</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-2-model-optimization-with-llm-compressor/techniques-for-model-compression-quantization-fp8-int8-and-sparsity.html">Techniques for Model Compression: Quantization (FP8, INT8) and Sparsity</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-2-model-optimization-with-llm-compressor/using-llm-compressor-to-create-optimized-modelcars-containerized-models.html">Using LLM Compressor to Create Optimized Models</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-2-model-optimization-with-llm-compressor/implementing-quantization-recipes-eg-smoothquant-gptq-to-reduce-gpu-memory-footprint.html">Implementing quantization recipes (e.g., SmoothQuant, GPTQ) to reduce GPU memory footprint</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-2-model-optimization-with-llm-compressor/accessing-validated-pre-optimized-models-eg-lean-llama-from-the-red-hat-model-catalog.html">Accessing Validated, Pre-optimized Models from the Red Hat Model Catalog</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../module-3-high-performance-serving-with-vllm-llm-d/module-3-high-performance-serving-with-vllm-llm-d.html">Module 3: High-Performance Serving with vLLM &amp; llm-d</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-3-high-performance-serving-with-vllm-llm-d/configuring-vllm-for-maximum-throughput-using-pagedattention-and-continuous-batching.html">Configuring vLLM for Maximum Throughput Using PagedAttention and Continuous Batching</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-3-high-performance-serving-with-vllm-llm-d/tuning-vllm-engine-arguments---gpu-memory-utilization-and---max-num-seqs.html">tuning vLLM engine arguments: <code>--gpu-memory-utilization</code> and <code>--max-num-seqs</code></a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-3-high-performance-serving-with-vllm-llm-d/deploying-distributed-inference-with-llm-d-to-separate-prefill-and-decode-phases-disaggregation.html">Deploying distributed inference with llm-d to separate prefill and decode phases (disaggregation)</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-3-high-performance-serving-with-vllm-llm-d/implementing-prefix-caching-to-reuse-key-value-kv-cache-for-rag-system-prompts.html">Implementing prefix caching to reuse Key-Value (KV) cache for RAG system prompts</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../module-4-standardized-rag-implementation/module-4-standardized-rag-implementation.html">Module 4: Standardized RAG Implementation</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-4-standardized-rag-implementation/ingesting-and-chunking-unstructured-enterprise-data-pdfs-using-docling.html">Ingesting and Chunking Unstructured Enterprise Data using Docling</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-4-standardized-rag-implementation/orchestrating-rag-workflows-using-the-llama-stack-api-for-standardized-response-generation-and-vector-io.html">Orchestrating RAG Workflows with the Llama Stack API</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-4-standardized-rag-implementation/integrating-vector-databases-eg-milvus-chroma-via-llama-stack-providers.html">Integrating Vector Databases via Llama Stack Providers</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-4-standardized-rag-implementation/deploying-the-rag-application-stack-using-gitops-and-argocd-for-repeatability.html">deploying the RAG application stack using GitOps and ArgoCD for repeatability</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../module-5-validation-benchmarking/module-5-validation-benchmarking.html">Module 5: Validation &amp; Benchmarking</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-5-validation-benchmarking/defining-service-level-objectives-slos-for-time-to-first-token-ttft-and-inter-token-latency-itl.html">Defining Service Level Objectives (SLOs) for Time To First Token (TTFT) and Inter-Token Latency (ITL)</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-5-validation-benchmarking/simulating-real-world-traffic-load-using-guidellm-to-validate-throughput-gains.html">Simulating real-world traffic load using GuideLLM to validate throughput gains</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-5-validation-benchmarking/comparing-performance-metrics-throughput-vs-latency.html">Comparing performance metrics (throughput vs. latency</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../appendix/appendix.html">Appendix</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Lean RAG Accelerator QuickStart Training</span>
    <span class="version">1.0.0</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Lean RAG Accelerator QuickStart Training</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1.0.0</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Lean RAG Accelerator QuickStart Training</a></li>
    <li><a href="module-1-inference-economics-rhoai-30-architecture.html">Module 1: Inference Economics &amp; RHOAI 3.0 Architecture</a></li>
    <li><a href="overview-of-the-red-hat-ai-30-ai-factory-approach-for-industrialized-model-serving.html">Overview of the Red Hat OpenShift AI 3.0 "AI Factory" Approach for Industrialized Model Serving</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Overview of the Red Hat OpenShift AI 3.0 "AI Factory" Approach for Industrialized Model Serving</h1>
<div class="sect1">
<h2 id="_introduction"><a class="anchor" href="#_introduction"></a>Introduction</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The Red Hat OpenShift AI (RHOAI) 3.0 "AI Factory" approach represents a significant evolution in the industrialization of model serving. It encapsulates a comprehensive strategy for deploying, managing, and scaling AI models in a production environment, ensuring efficiency, reliability, and cost-effectiveness.</p>
</div>
<div class="paragraph">
<p>The AI Factory transforms ad-hoc model deployments into industrialized, repeatable processes that scale from development to production.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_core_principles_of_the_ai_factory"><a class="anchor" href="#_core_principles_of_the_ai_factory"></a>Core Principles of the AI Factory</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The AI Factory is built on several core principles:</p>
</div>
<div class="sect2">
<h3 id="_standardization"><a class="anchor" href="#_standardization"></a>Standardization</h3>
<div class="paragraph">
<p>The AI Factory promotes the use of standardized components and processes to ensure consistency and repeatability in model serving. This includes:
- Containerized models (ModelCars)
- Standardized APIs (Llama Stack, KServe)
- Uniform data ingestion pipelines
- Consistent deployment patterns</p>
</div>
<div class="paragraph">
<p><strong>Benefits:</strong>
- Reduced deployment time
- Lower operational overhead
- Easier maintenance and updates
- Consistent performance across environments</p>
</div>
</div>
<div class="sect2">
<h3 id="_automation"><a class="anchor" href="#_automation"></a>Automation</h3>
<div class="paragraph">
<p>Automation is a key pillar of the AI Factory. It involves automating:
- Model deployment and scaling
- Monitoring and alerting
- Resource management
- Workflow orchestration</p>
</div>
<div class="paragraph">
<p><strong>Tools:</strong>
- GitOps (ArgoCD)
- Kubernetes operators
- CI/CD pipelines
- Automated testing and validation</p>
</div>
<div class="paragraph">
<p><strong>Benefits:</strong>
- Reduced manual intervention
- Fewer human errors
- Faster deployments
- Consistent configurations</p>
</div>
</div>
<div class="sect2">
<h3 id="_scalability"><a class="anchor" href="#_scalability"></a>Scalability</h3>
<div class="paragraph">
<p>The AI Factory is designed to handle the scalability demands of production AI workloads. It leverages:
- Distributed systems architecture
- Horizontal scaling capabilities
- Load balancing and routing
- Resource optimization</p>
</div>
<div class="paragraph">
<p><strong>Benefits:</strong>
- Handle increased request volumes
- Scale resources dynamically
- Maintain performance under load
- Cost-effective scaling</p>
</div>
</div>
<div class="sect2">
<h3 id="_observability"><a class="anchor" href="#_observability"></a>Observability</h3>
<div class="paragraph">
<p>The AI Factory emphasizes the importance of monitoring and logging in production environments. This includes:
- Real-time performance tracking
- Resource utilization monitoring
- Error tracking and alerting
- Cost monitoring and optimization</p>
</div>
<div class="paragraph">
<p><strong>Benefits:</strong>
- Proactive issue detection
- Performance optimization
- Cost visibility
- Continuous improvement</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_key_components_of_the_ai_factory"><a class="anchor" href="#_key_components_of_the_ai_factory"></a>Key Components of the AI Factory</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The AI Factory comprises several key components that work together to provide industrialized model serving:</p>
</div>
<div class="sect2">
<h3 id="_model_catalog"><a class="anchor" href="#_model_catalog"></a>Model Catalog</h3>
<div class="paragraph">
<p>A repository of validated, pre-optimized models that can be directly used for inference. This eliminates the need for custom model optimization for each deployment.</p>
</div>
<div class="paragraph">
<p><strong>Features:</strong>
- Pre-validated models tested with RHOAI 3.0
- Multiple quantization variants (INT4, INT8, FP8)
- OCI container images (ModelCars)
- Version management</p>
</div>
<div class="paragraph">
<p><strong>Benefits:</strong>
- Faster time to deployment
- Reduced optimization effort
- Proven model performance
- Consistent model quality</p>
</div>
</div>
<div class="sect2">
<h3 id="_llm_compressor"><a class="anchor" href="#_llm_compressor"></a>LLM Compressor</h3>
<div class="paragraph">
<p>A tool for model compression, enabling the creation of optimized "ModelCars" (containerized models) with reduced memory footprint. It uses techniques like quantization and sparsity to achieve this.</p>
</div>
<div class="paragraph">
<p><strong>Features:</strong>
- Kubernetes-native (QuantizationRecipe CRD)
- Multiple quantization methods (SmoothQuant, GPTQ)
- Integration with RHOAI 3.0 infrastructure
- Automated optimization workflows</p>
</div>
<div class="paragraph">
<p><strong>Benefits:</strong>
- 50-75% memory reduction
- Faster inference
- Lower infrastructure costs
- Production-ready models</p>
</div>
</div>
<div class="sect2">
<h3 id="_vllm_llm_d"><a class="anchor" href="#_vllm_llm_d"></a>vLLM &amp; llm-d</h3>
<div class="paragraph">
<p>These are the core inference components in the AI Factory:</p>
</div>
<div class="paragraph">
<p><strong>vLLM</strong>: The high-performance inference engine optimized for maximum throughput, providing innovations like:
- PagedAttention for efficient memory management
- Speculative decoding for faster generation
- Tensor parallelism for multi-GPU scaling</p>
</div>
<div class="paragraph">
<p><strong>llm-d</strong>: The cloud-native orchestrator that enhances vLLM by enabling:
- Distributed inference (prefill/decode disaggregation)
- KV cache-aware routing
- Kubernetes-native elasticity
- Production telemetry</p>
</div>
<div class="paragraph">
<p><strong>Together</strong>: They create a championship-ready inference system that scales from single-node to global deployments.</p>
</div>
</div>
<div class="sect2">
<h3 id="_llama_stack_api"><a class="anchor" href="#_llama_stack_api"></a>Llama Stack API</h3>
<div class="paragraph">
<p>This API provides a standardized interface for RAG deployments, integrating:
- LLM inference
- Semantic retrieval
- Vector database operations
- RAG workflow orchestration</p>
</div>
<div class="paragraph">
<p><strong>Features:</strong>
- Unified API for all RAG operations
- Embedded vector stores for "Lean RAG"
- OpenAI-compatible endpoints
- Production-ready deployment</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_ai_factory_workflow"><a class="anchor" href="#_ai_factory_workflow"></a>AI Factory Workflow</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_development_phase"><a class="anchor" href="#_development_phase"></a>Development Phase</h3>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Model Selection</strong>: Choose from Model Catalog or optimize with LLM Compressor</p>
</li>
<li>
<p><strong>Configuration</strong>: Define deployment configuration</p>
</li>
<li>
<p><strong>Testing</strong>: Validate in development environment</p>
</li>
<li>
<p><strong>Optimization</strong>: Tune for performance and cost</p>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="_deployment_phase"><a class="anchor" href="#_deployment_phase"></a>Deployment Phase</h3>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>GitOps</strong>: Deploy via ArgoCD from Git repository</p>
</li>
<li>
<p><strong>Automation</strong>: Automated scaling and resource management</p>
</li>
<li>
<p><strong>Monitoring</strong>: Real-time observability</p>
</li>
<li>
<p><strong>Validation</strong>: Automated testing and quality checks</p>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="_production_phase"><a class="anchor" href="#_production_phase"></a>Production Phase</h3>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Scaling</strong>: Dynamic scaling based on demand</p>
</li>
<li>
<p><strong>Monitoring</strong>: Continuous performance tracking</p>
</li>
<li>
<p><strong>Optimization</strong>: Continuous improvement</p>
</li>
<li>
<p><strong>Maintenance</strong>: Automated updates and patches</p>
</li>
</ol>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_benefits_of_the_ai_factory_approach"><a class="anchor" href="#_benefits_of_the_ai_factory_approach"></a>Benefits of the AI Factory Approach</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_for_developers"><a class="anchor" href="#_for_developers"></a>For Developers</h3>
<div class="ulist">
<ul>
<li>
<p>Faster development cycles</p>
</li>
<li>
<p>Standardized tooling and processes</p>
</li>
<li>
<p>Pre-validated components</p>
</li>
<li>
<p>Reduced complexity</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_for_operations"><a class="anchor" href="#_for_operations"></a>For Operations</h3>
<div class="ulist">
<ul>
<li>
<p>Automated deployments</p>
</li>
<li>
<p>Consistent configurations</p>
</li>
<li>
<p>Better observability</p>
</li>
<li>
<p>Reduced operational overhead</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_for_business"><a class="anchor" href="#_for_business"></a>For Business</h3>
<div class="ulist">
<ul>
<li>
<p>Faster time to market</p>
</li>
<li>
<p>Lower infrastructure costs</p>
</li>
<li>
<p>Better scalability</p>
</li>
<li>
<p>Reduced risk</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_comparison_ai_factory_vs_ad_hoc_deployments"><a class="anchor" href="#_comparison_ai_factory_vs_ad_hoc_deployments"></a>Comparison: AI Factory vs. Ad-Hoc Deployments</h2>
<div class="sectionbody">
<div class="paragraph">
<p>| Aspect | Ad-Hoc Deployment | AI Factory |
|--------|-------------------|------------|
| <strong>Setup Time</strong> | Days to weeks | Hours |
| <strong>Consistency</strong> | Variable | Standardized |
| <strong>Scalability</strong> | Manual | Automated |
| <strong>Observability</strong> | Limited | Comprehensive |
| <strong>Cost</strong> | Higher (inefficient) | Lower (optimized) |
| <strong>Maintenance</strong> | High effort | Automated |</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_integration_with_rhoai_3_0"><a class="anchor" href="#_integration_with_rhoai_3_0"></a>Integration with RHOAI 3.0</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The AI Factory is deeply integrated with RHOAI 3.0:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Platform Integration</strong>: Native Kubernetes operators</p>
</li>
<li>
<p><strong>Model Registry</strong>: Integrated model catalog</p>
</li>
<li>
<p><strong>Monitoring</strong>: Built-in observability</p>
</li>
<li>
<p><strong>Security</strong>: Enterprise-grade security features</p>
</li>
<li>
<p><strong>Compliance</strong>: Meets enterprise requirements</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_key_takeaways"><a class="anchor" href="#_key_takeaways"></a>Key Takeaways</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>AI Factory provides industrialized model serving</p>
</li>
<li>
<p>Standardization, automation, scalability, and observability are core principles</p>
</li>
<li>
<p>Components work together for end-to-end solution</p>
</li>
<li>
<p>Reduces time to deployment and operational overhead</p>
</li>
<li>
<p>Scales from development to production seamlessly</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_next_steps"><a class="anchor" href="#_next_steps"></a>Next Steps</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Learn about the RHOAI 3.0 inference stack components</p>
</li>
<li>
<p>Explore model optimization with LLM Compressor</p>
</li>
<li>
<p>Understand RAG deployment with Llama Stack</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_related_resources"><a class="anchor" href="#_related_resources"></a>Related Resources</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p><a href="mapping-the-rhoai-30-inference-stack-vllm-llm-d-and-kserve-integration.adoc">RHOAI 3.0 Inference Stack</a> - Component details</p>
</li>
<li>
<p><a href="../../module-2-model-optimization-with-llm-compressor/using-llm-compressor-to-create-optimized-modelcars-containerized-models.adoc">LLM Compressor</a> - Model optimization</p>
</li>
<li>
<p><a href="../../lean-rag-accelerator/docs/architecture.md">Architecture Documentation</a> - Technical details</p>
</li>
</ul>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="identifying-critical-gpu-underutilization-and-the-role-of-memory-management-in-inference-costs.html">Identifying "Critical GPU Underutilization" and the Role of Memory Management in Inference Costs</a></span>
  <span class="next"><a href="mapping-the-rhoai-30-inference-stack-vllm-llm-d-and-kserve-integration.html">Mapping the RHOAI 3.0 inference stack: vLLM, llm-d, and KServe integration</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
