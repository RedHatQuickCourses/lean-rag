<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Module 3: High-Performance Serving with vLLM &amp; llm-d :: Lean RAG Accelerator QuickStart Training</title>
    <link rel="prev" href="../module-2-model-optimization-with-llm-compressor/accessing-validated-pre-optimized-models-eg-lean-llama-from-the-red-hat-model-catalog.html">
    <link rel="next" href="configuring-vllm-for-maximum-throughput-using-pagedattention-and-continuous-batching.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">Lean RAG Accelerator QuickStart Training</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="lean-rag" data-version="1.0.0">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Lean RAG Accelerator QuickStart Training</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Home</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../LABENV/index.html">Lab Environment</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../module-1-inference-economics-rhoai-30-architecture/module-1-inference-economics-rhoai-30-architecture.html">Module 1: Inference Economics &amp; RHOAI 3.0 Architecture</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-1-inference-economics-rhoai-30-architecture/analysis-of-the-impossible-trinity-in-llm-deployments-balancing-accuracy-latency-and-cost.html">Analysis of the "Impossible Trinity" in LLM Deployments: Balancing Accuracy, Latency, and Cost</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-1-inference-economics-rhoai-30-architecture/identifying-critical-gpu-underutilization-and-the-role-of-memory-management-in-inference-costs.html">Identifying "Critical GPU Underutilization" and the Role of Memory Management in Inference Costs</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-1-inference-economics-rhoai-30-architecture/overview-of-the-red-hat-ai-30-ai-factory-approach-for-industrialized-model-serving.html">Overview of the Red Hat OpenShift AI 3.0 "AI Factory" Approach for Industrialized Model Serving</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-1-inference-economics-rhoai-30-architecture/mapping-the-rhoai-30-inference-stack-vllm-llm-d-and-kserve-integration.html">Mapping the RHOAI 3.0 inference stack: vLLM, llm-d, and KServe integration</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../module-2-model-optimization-with-llm-compressor/module-2-model-optimization-with-llm-compressor.html">Module 2: Model Optimization with LLM Compressor</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-2-model-optimization-with-llm-compressor/techniques-for-model-compression-quantization-fp8-int8-and-sparsity.html">Techniques for Model Compression: Quantization (FP8, INT8) and Sparsity</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-2-model-optimization-with-llm-compressor/using-llm-compressor-to-create-optimized-modelcars-containerized-models.html">Using LLM Compressor to Create Optimized Models</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-2-model-optimization-with-llm-compressor/implementing-quantization-recipes-eg-smoothquant-gptq-to-reduce-gpu-memory-footprint.html">Implementing quantization recipes (e.g., SmoothQuant, GPTQ) to reduce GPU memory footprint</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-2-model-optimization-with-llm-compressor/accessing-validated-pre-optimized-models-eg-lean-llama-from-the-red-hat-model-catalog.html">Accessing Validated, Pre-optimized Models from the Red Hat Model Catalog</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="module-3-high-performance-serving-with-vllm-llm-d.html">Module 3: High-Performance Serving with vLLM &amp; llm-d</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="configuring-vllm-for-maximum-throughput-using-pagedattention-and-continuous-batching.html">Configuring vLLM for Maximum Throughput Using PagedAttention and Continuous Batching</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="tuning-vllm-engine-arguments---gpu-memory-utilization-and---max-num-seqs.html">tuning vLLM engine arguments: <code>--gpu-memory-utilization</code> and <code>--max-num-seqs</code></a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="deploying-distributed-inference-with-llm-d-to-separate-prefill-and-decode-phases-disaggregation.html">Deploying distributed inference with llm-d to separate prefill and decode phases (disaggregation)</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="implementing-prefix-caching-to-reuse-key-value-kv-cache-for-rag-system-prompts.html">Implementing prefix caching to reuse Key-Value (KV) cache for RAG system prompts</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../module-4-standardized-rag-implementation/module-4-standardized-rag-implementation.html">Module 4: Standardized RAG Implementation</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-4-standardized-rag-implementation/ingesting-and-chunking-unstructured-enterprise-data-pdfs-using-docling.html">Ingesting and Chunking Unstructured Enterprise Data using Docling</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-4-standardized-rag-implementation/orchestrating-rag-workflows-using-the-llama-stack-api-for-standardized-response-generation-and-vector-io.html">Orchestrating RAG Workflows with the Llama Stack API</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-4-standardized-rag-implementation/integrating-vector-databases-eg-milvus-chroma-via-llama-stack-providers.html">Integrating Vector Databases via Llama Stack Providers</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-4-standardized-rag-implementation/deploying-the-rag-application-stack-using-gitops-and-argocd-for-repeatability.html">deploying the RAG application stack using GitOps and ArgoCD for repeatability</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../module-5-validation-benchmarking/module-5-validation-benchmarking.html">Module 5: Validation &amp; Benchmarking</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-5-validation-benchmarking/defining-service-level-objectives-slos-for-time-to-first-token-ttft-and-inter-token-latency-itl.html">Defining Service Level Objectives (SLOs) for Time To First Token (TTFT) and Inter-Token Latency (ITL)</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-5-validation-benchmarking/simulating-real-world-traffic-load-using-guidellm-to-validate-throughput-gains.html">Simulating real-world traffic load using GuideLLM to validate throughput gains</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-5-validation-benchmarking/comparing-performance-metrics-throughput-vs-latency.html">Comparing performance metrics (throughput vs. latency</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../appendix/appendix.html">Appendix</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Lean RAG Accelerator QuickStart Training</span>
    <span class="version">1.0.0</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Lean RAG Accelerator QuickStart Training</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1.0.0</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Lean RAG Accelerator QuickStart Training</a></li>
    <li><a href="module-3-high-performance-serving-with-vllm-llm-d.html">Module 3: High-Performance Serving with vLLM &amp; llm-d</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Module 3: High-Performance Serving with vLLM &amp; llm-d</h1>
<div class="sect1">
<h2 id="_introduction"><a class="anchor" href="#_introduction"></a>Introduction</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This module covers how to deploy optimized models for maximum throughput using RHOAI 3.0&#8217;s inference stack: vLLM (the high-performance engine) and llm-d (the cloud-native orchestrator). You&#8217;ll learn how to achieve 2-4x throughput improvement and 85-90% GPU utilization.</p>
</div>
<div class="paragraph">
<p><strong>Key Learning Objectives:</strong>
- Configure vLLM for maximum throughput
- Tune vLLM engine arguments for optimal performance
- Deploy distributed inference with llm-d
- Implement prefix caching for RAG systems</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_module_overview"><a class="anchor" href="#_module_overview"></a>Module Overview</h2>
<div class="sectionbody">
<div class="paragraph">
<p>After optimizing your model (Module 2), the next step is deploying it for high-performance serving. RHOAI 3.0&#8217;s inference stack provides:
- <strong>vLLM</strong>: High-performance inference engine with PagedAttention
- <strong>llm-d</strong>: Cloud-native orchestrator for production-scale deployments
- <strong>KServe</strong>: Kubernetes-native serving framework</p>
</div>
<div class="sect2">
<h3 id="_understanding_the_ecosystem_engine_versus_platform"><a class="anchor" href="#_understanding_the_ecosystem_engine_versus_platform"></a>Understanding the Ecosystem: Engine versus Platform</h3>
<div class="paragraph">
<p><strong>vLLM</strong> is the high-performance Formula 1 car—the state-of-the-art inference engine that provides raw speed and efficiency through innovations like PagedAttention, speculative decoding, and tensor parallelism.</p>
</div>
<div class="paragraph">
<p><strong>llm-d</strong> is the pit crew, race strategist, and telemetry system combined—a cloud-native distributed inference framework that orchestrates vLLM to manage complex production workloads.</p>
</div>
<div class="paragraph">
<p><strong>Important</strong>: vLLM and llm-d are designed to work together, not as alternatives. llm-d does not replace vLLM—it enhances it.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_topics_covered"><a class="anchor" href="#_topics_covered"></a>Topics Covered</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_section_3_1_configuring_vllm_for_maximum_throughput"><a class="anchor" href="#_section_3_1_configuring_vllm_for_maximum_throughput"></a>Section 3.1: Configuring vLLM for Maximum Throughput</h3>
<div class="paragraph">
<p>Learn how to configure vLLM using PagedAttention and Continuous Batching:
- PagedAttention architecture
- Continuous batching mechanism
- vLLM ServingRuntime configuration
- Optimization arguments
- Deployment steps</p>
</div>
<div class="paragraph">
<p><strong>Duration</strong>: 90-120 minutes</p>
</div>
</div>
<div class="sect2">
<h3 id="_section_3_2_tuning_vllm_engine_arguments"><a class="anchor" href="#_section_3_2_tuning_vllm_engine_arguments"></a>Section 3.2: Tuning vLLM Engine Arguments</h3>
<div class="paragraph">
<p>Deep dive into vLLM parameter tuning:
- GPU memory utilization
- Concurrent request handling
- Advanced tuning parameters
- Performance profiling
- Iterative optimization</p>
</div>
<div class="paragraph">
<p><strong>Duration</strong>: 75-90 minutes</p>
</div>
</div>
<div class="sect2">
<h3 id="_section_3_3_distributed_inference_with_llm_d"><a class="anchor" href="#_section_3_3_distributed_inference_with_llm_d"></a>Section 3.3: Distributed Inference with llm-d</h3>
<div class="paragraph">
<p>Deploy llm-d for production-scale orchestration:
- Prefill and decode phases
- Disaggregation architecture
- llm-d configuration
- When to use llm-d
- Deployment examples</p>
</div>
<div class="paragraph">
<p><strong>Duration</strong>: 90-120 minutes</p>
</div>
</div>
<div class="sect2">
<h3 id="_section_3_4_prefix_caching_for_rag"><a class="anchor" href="#_section_3_4_prefix_caching_for_rag"></a>Section 3.4: Prefix Caching for RAG</h3>
<div class="paragraph">
<p>Implement prefix caching for RAG systems:
- KV cache reuse concepts
- Prefix caching configuration
- RAG-specific benefits
- Performance impact</p>
</div>
<div class="paragraph">
<p><strong>Duration</strong>: 60-75 minutes</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_when_to_use_vllm_alone_vs_vllm_llm_d"><a class="anchor" href="#_when_to_use_vllm_alone_vs_vllm_llm_d"></a>When to Use vLLM Alone vs. vLLM + llm-d</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_start_with_vllm_alone_when"><a class="anchor" href="#_start_with_vllm_alone_when"></a>Start with vLLM Alone When:</h3>
<div class="ulist">
<ul>
<li>
<p>Single node deployments</p>
</li>
<li>
<p>Well-tuned multi-GPU clusters</p>
</li>
<li>
<p>Simpler workloads with predictable traffic patterns</p>
</li>
<li>
<p>Development and testing environments</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_add_llm_d_when_you_need"><a class="anchor" href="#_add_llm_d_when_you_need"></a>Add llm-d When You Need:</h3>
<div class="ulist">
<ul>
<li>
<p>Independent scaling of prefill and decode workers</p>
</li>
<li>
<p>MoE model support (models too large for single GPU)</p>
</li>
<li>
<p>KV cache-aware routing for RAG systems</p>
</li>
<li>
<p>Kubernetes-native elasticity with KEDA/ArgoCD</p>
</li>
<li>
<p>Production telemetry and observability</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_prerequisites"><a class="anchor" href="#_prerequisites"></a>Prerequisites</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Before starting this module, ensure you have:
- Completed Module 2 (have an optimized model ready)
- Red Hat OpenShift AI 3.0 installed
- KServe configured
- Access to GPU nodes
- Basic Kubernetes knowledge</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_expected_outcomes"><a class="anchor" href="#_expected_outcomes"></a>Expected Outcomes</h2>
<div class="sectionbody">
<div class="paragraph">
<p>By the end of this module, you will be able to:
- Configure vLLM for maximum throughput
- Tune vLLM parameters for optimal performance
- Deploy llm-d for distributed inference
- Implement prefix caching
- Achieve 2-4x throughput improvement</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_module_structure"><a class="anchor" href="#_module_structure"></a>Module Structure</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This module follows a practical, hands-on approach:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Configuration</strong>: Learn vLLM setup and optimization</p>
</li>
<li>
<p><strong>Tuning</strong>: Iteratively optimize performance</p>
</li>
<li>
<p><strong>Scaling</strong>: Add llm-d for production capabilities</p>
</li>
<li>
<p><strong>Optimization</strong>: Implement advanced features like prefix caching</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_next_steps"><a class="anchor" href="#_next_steps"></a>Next Steps</h2>
<div class="sectionbody">
<div class="paragraph">
<p>After completing this module:
- Proceed to Module 4 to learn about RAG implementation
- Deploy your optimized model with vLLM/llm-d
- Measure performance improvements</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_related_resources"><a class="anchor" href="#_related_resources"></a>Related Resources</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p><a href="module-2-model-optimization-with-llm-compressor.adoc">Module 2: Model Optimization</a> - Previous module</p>
</li>
<li>
<p><a href="module-4-standardized-rag-implementation.adoc">Module 4: Standardized RAG</a> - Next module</p>
</li>
<li>
<p><a href="../../lean-rag-accelerator/examples/02-inference-serving/README.md">Inference Serving Examples</a> - Quickstart code</p>
</li>
</ul>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="../module-2-model-optimization-with-llm-compressor/accessing-validated-pre-optimized-models-eg-lean-llama-from-the-red-hat-model-catalog.html">Accessing Validated, Pre-optimized Models from the Red Hat Model Catalog</a></span>
  <span class="next"><a href="configuring-vllm-for-maximum-throughput-using-pagedattention-and-continuous-batching.html">Configuring vLLM for Maximum Throughput Using PagedAttention and Continuous Batching</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
