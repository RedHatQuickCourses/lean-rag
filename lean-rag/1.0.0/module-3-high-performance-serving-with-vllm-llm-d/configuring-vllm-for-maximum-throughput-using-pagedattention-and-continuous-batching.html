<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Configuring vLLM for Maximum Throughput Using PagedAttention and Continuous Batching :: Lean RAG Accelerator QuickStart Training</title>
    <link rel="prev" href="module-3-high-performance-serving-with-vllm-llm-d.html">
    <link rel="next" href="tuning-vllm-engine-arguments---gpu-memory-utilization-and---max-num-seqs.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">Lean RAG Accelerator QuickStart Training</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="lean-rag" data-version="1.0.0">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Lean RAG Accelerator QuickStart Training</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Home</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../LABENV/index.html">Lab Environment</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../module-1-inference-economics-rhoai-30-architecture/module-1-inference-economics-rhoai-30-architecture.html">Module 1: Inference Economics &amp; RHOAI 3.0 Architecture</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-1-inference-economics-rhoai-30-architecture/analysis-of-the-impossible-trinity-in-llm-deployments-balancing-accuracy-latency-and-cost.html">Analysis of the "Impossible Trinity" in LLM Deployments: Balancing Accuracy, Latency, and Cost</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-1-inference-economics-rhoai-30-architecture/identifying-critical-gpu-underutilization-and-the-role-of-memory-management-in-inference-costs.html">Identifying "Critical GPU Underutilization" and the Role of Memory Management in Inference Costs</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-1-inference-economics-rhoai-30-architecture/overview-of-the-red-hat-ai-30-ai-factory-approach-for-industrialized-model-serving.html">Overview of the Red Hat OpenShift AI 3.0 "AI Factory" Approach for Industrialized Model Serving</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-1-inference-economics-rhoai-30-architecture/mapping-the-rhoai-30-inference-stack-vllm-llm-d-and-kserve-integration.html">Mapping the RHOAI 3.0 inference stack: vLLM, llm-d, and KServe integration</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../module-2-model-optimization-with-llm-compressor/module-2-model-optimization-with-llm-compressor.html">Module 2: Model Optimization with LLM Compressor</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-2-model-optimization-with-llm-compressor/techniques-for-model-compression-quantization-fp8-int8-and-sparsity.html">Techniques for Model Compression: Quantization (FP8, INT8) and Sparsity</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-2-model-optimization-with-llm-compressor/using-llm-compressor-to-create-optimized-modelcars-containerized-models.html">Using LLM Compressor to Create Optimized Models</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-2-model-optimization-with-llm-compressor/implementing-quantization-recipes-eg-smoothquant-gptq-to-reduce-gpu-memory-footprint.html">Implementing quantization recipes (e.g., SmoothQuant, GPTQ) to reduce GPU memory footprint</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-2-model-optimization-with-llm-compressor/accessing-validated-pre-optimized-models-eg-lean-llama-from-the-red-hat-model-catalog.html">Accessing Validated, Pre-optimized Models from the Red Hat Model Catalog</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="module-3-high-performance-serving-with-vllm-llm-d.html">Module 3: High-Performance Serving with vLLM &amp; llm-d</a>
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="configuring-vllm-for-maximum-throughput-using-pagedattention-and-continuous-batching.html">Configuring vLLM for Maximum Throughput Using PagedAttention and Continuous Batching</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="tuning-vllm-engine-arguments---gpu-memory-utilization-and---max-num-seqs.html">tuning vLLM engine arguments: <code>--gpu-memory-utilization</code> and <code>--max-num-seqs</code></a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="deploying-distributed-inference-with-llm-d-to-separate-prefill-and-decode-phases-disaggregation.html">Deploying distributed inference with llm-d to separate prefill and decode phases (disaggregation)</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="implementing-prefix-caching-to-reuse-key-value-kv-cache-for-rag-system-prompts.html">Implementing prefix caching to reuse Key-Value (KV) cache for RAG system prompts</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../module-4-standardized-rag-implementation/module-4-standardized-rag-implementation.html">Module 4: Standardized RAG Implementation</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-4-standardized-rag-implementation/ingesting-and-chunking-unstructured-enterprise-data-pdfs-using-docling.html">Ingesting and Chunking Unstructured Enterprise Data using Docling</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-4-standardized-rag-implementation/orchestrating-rag-workflows-using-the-llama-stack-api-for-standardized-response-generation-and-vector-io.html">Orchestrating RAG Workflows with the Llama Stack API</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-4-standardized-rag-implementation/integrating-vector-databases-eg-milvus-chroma-via-llama-stack-providers.html">Integrating Vector Databases via Llama Stack Providers</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-4-standardized-rag-implementation/deploying-the-rag-application-stack-using-gitops-and-argocd-for-repeatability.html">deploying the RAG application stack using GitOps and ArgoCD for repeatability</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../module-5-validation-benchmarking/module-5-validation-benchmarking.html">Module 5: Validation &amp; Benchmarking</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-5-validation-benchmarking/defining-service-level-objectives-slos-for-time-to-first-token-ttft-and-inter-token-latency-itl.html">Defining Service Level Objectives (SLOs) for Time To First Token (TTFT) and Inter-Token Latency (ITL)</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-5-validation-benchmarking/simulating-real-world-traffic-load-using-guidellm-to-validate-throughput-gains.html">Simulating real-world traffic load using GuideLLM to validate throughput gains</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-5-validation-benchmarking/comparing-performance-metrics-throughput-vs-latency.html">Comparing performance metrics (throughput vs. latency</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../appendix/appendix.html">Appendix</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Lean RAG Accelerator QuickStart Training</span>
    <span class="version">1.0.0</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Lean RAG Accelerator QuickStart Training</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1.0.0</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Lean RAG Accelerator QuickStart Training</a></li>
    <li><a href="module-3-high-performance-serving-with-vllm-llm-d.html">Module 3: High-Performance Serving with vLLM &amp; llm-d</a></li>
    <li><a href="configuring-vllm-for-maximum-throughput-using-pagedattention-and-continuous-batching.html">Configuring vLLM for Maximum Throughput Using PagedAttention and Continuous Batching</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Configuring vLLM for Maximum Throughput Using PagedAttention and Continuous Batching</h1>
<div class="sect1">
<h2 id="_introduction"><a class="anchor" href="#_introduction"></a>Introduction</h2>
<div class="sectionbody">
<div class="paragraph">
<p>vLLM is the high-performance inference engine in RHOAI 3.0&#8217;s inference stack. This section covers configuring vLLM for maximum throughput using its key innovations: PagedAttention and Continuous Batching.</p>
</div>
<div class="paragraph">
<p><strong>Expected Results:</strong>
- 2-4x throughput improvement over baseline
- 85-90% GPU utilization (up from &lt;40%)
- &lt;100ms Time to First Token (TTFT)
- 10-20 concurrent requests per GPU</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_understanding_pagedattention"><a class="anchor" href="#_understanding_pagedattention"></a>Understanding PagedAttention</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_what_is_pagedattention"><a class="anchor" href="#_what_is_pagedattention"></a>What is PagedAttention?</h3>
<div class="paragraph">
<p>PagedAttention is vLLM&#8217;s memory management system that eliminates memory fragmentation in KV cache. It works like an operating system&#8217;s virtual memory:</p>
</div>
<div class="paragraph">
<p><strong>Traditional Approach (Problem):</strong>
- Fixed-size memory blocks per request
- Blocks cannot be reused
- Memory fragmentation prevents batching
- GPU memory appears "full" but is fragmented</p>
</div>
<div class="paragraph">
<p><strong>PagedAttention (Solution):</strong>
- Divides KV cache into fixed-size pages (16KB blocks)
- Pages can be allocated/deallocated dynamically
- Eliminates fragmentation
- Enables efficient continuous batching</p>
</div>
</div>
<div class="sect2">
<h3 id="_how_pagedattention_works"><a class="anchor" href="#_how_pagedattention_works"></a>How PagedAttention Works</h3>
<div class="paragraph">
<p><strong>Memory Layout:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">Traditional (Fragmented):
[Request1: 2GB][Free: 0.5GB][Request2: 2GB][Free: 0.3GB][Request3: 2GB]
                    ↑ Cannot allocate 2GB for Request4 (fragmented)

PagedAttention (Paged):
[Page1][Page2][Page3][Page4][Page5][Page6][Page7][Page8]...
  ↑ Pages can be allocated/deallocated dynamically
  ↑ No fragmentation - any free page can be used</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Benefits:</strong>
- <strong>No Fragmentation</strong>: Memory can be used efficiently
- <strong>Dynamic Allocation</strong>: Pages allocated as needed
- <strong>Better Batching</strong>: Can batch more requests
- <strong>Higher Throughput</strong>: 2-4x improvement</p>
</div>
</div>
<div class="sect2">
<h3 id="_pagedattention_configuration"><a class="anchor" href="#_pagedattention_configuration"></a>PagedAttention Configuration</h3>
<div class="paragraph">
<p><strong>PagedAttention is enabled by default in vLLM</strong> - no explicit configuration needed. However, you can tune related parameters:</p>
</div>
<div class="paragraph">
<p><strong>Key Parameters:</strong>
- <code>--block-size</code>: Size of memory blocks (default: 16, typically don&#8217;t change)
- <code>--gpu-memory-utilization</code>: How much GPU memory to use (0.9 recommended)
- <code>--max-num-seqs</code>: Maximum concurrent sequences (affects batching)</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_understanding_continuous_batching"><a class="anchor" href="#_understanding_continuous_batching"></a>Understanding Continuous Batching</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_what_is_continuous_batching"><a class="anchor" href="#_what_is_continuous_batching"></a>What is Continuous Batching?</h3>
<div class="paragraph">
<p>Continuous Batching (also called "iteration-level batching") allows vLLM to:
- Add new requests to batch as GPU processes
- Remove completed requests from batch
- Keep GPU constantly busy
- Process requests of different lengths efficiently</p>
</div>
<div class="paragraph">
<p><strong>Traditional Batching (Problem):</strong>
- Wait for batch to fill before processing
- Process entire batch together
- GPU idle while waiting for new requests
- Inefficient for variable-length requests</p>
</div>
<div class="paragraph">
<p><strong>Continuous Batching (Solution):</strong>
- Add requests dynamically as they arrive
- Remove completed requests immediately
- GPU always processing
- Efficient for variable-length requests</p>
</div>
</div>
<div class="sect2">
<h3 id="_how_continuous_batching_works"><a class="anchor" href="#_how_continuous_batching_works"></a>How Continuous Batching Works</h3>
<div class="paragraph">
<p><strong>Timeline Example:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">Traditional Batching:
Time 0: [Request1, Request2] → Process
Time 1: [Request3, Request4] → Wait for batch
Time 2: [Request3, Request4] → Process
Time 3: [Request5] → Wait for more requests
Time 4: [Request5, Request6] → Process
        ↑ GPU idle 40% of time

Continuous Batching:
Time 0: [Request1, Request2] → Process
Time 1: [Request1, Request2, Request3] → Add Request3, continue
Time 2: [Request2, Request3, Request4] → Request1 done, add Request4
Time 3: [Request3, Request4, Request5] → Request2 done, add Request5
        ↑ GPU busy 95% of time</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Benefits:</strong>
- <strong>No Waiting</strong>: GPU always processing
- <strong>Dynamic Batching</strong>: Adapts to request arrival
- <strong>Better Utilization</strong>: 85-90% vs. &lt;40%
- <strong>Lower Latency</strong>: No waiting for batch to fill</p>
</div>
</div>
<div class="sect2">
<h3 id="_continuous_batching_configuration"><a class="anchor" href="#_continuous_batching_configuration"></a>Continuous Batching Configuration</h3>
<div class="paragraph">
<p><strong>Continuous Batching is enabled by default in vLLM</strong> - no explicit flag needed. Tune with:</p>
</div>
<div class="paragraph">
<p><strong>Key Parameters:</strong>
- <code>--max-num-seqs</code>: Maximum concurrent sequences (default: 256, tune based on GPU memory)
- <code>--gpu-memory-utilization</code>: How much memory to use (0.9 for maximum throughput)</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_vllm_configuration_for_maximum_throughput"><a class="anchor" href="#_vllm_configuration_for_maximum_throughput"></a>vLLM Configuration for Maximum Throughput</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_basic_vllm_servingruntime_configuration"><a class="anchor" href="#_basic_vllm_servingruntime_configuration"></a>Basic vLLM ServingRuntime Configuration</h3>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: vllm-runtime
  namespace: default
spec:
  supportedModelFormats:
    - name: llama
      version: "1"
      autoSelect: true

  containers:
    - name: vllm
      image: &lt;vllm-image&gt;  # e.g., vllm/vllm-openai:latest
      args:
        - --model
        - /mnt/models
        - --host
        - "0.0.0.0"
        - --port
        - "8000"
        # Key optimization arguments:
        - --gpu-memory-utilization
        - "0.9"  # Use 90% of GPU memory
        - --kv-cache-dtype
        - "fp8"  # FP8 KV cache (memory efficient)
        - --tensor-parallel-size
        - "1"  # Adjust for multi-GPU
        - --max-model-len
        - "8192"  # Context length

      resources:
        requests:
          memory: "16Gi"
          cpu: "4"
          nvidia.com/gpu: 1
        limits:
          memory: "32Gi"
          cpu: "8"
          nvidia.com/gpu: 1

      volumeMounts:
        - name: model-storage
          mountPath: /mnt/models
          readOnly: true
  volumes:
    - name: model-storage
      persistentVolumeClaim:
        claimName: model-storage-pvc</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_key_optimization_arguments"><a class="anchor" href="#_key_optimization_arguments"></a>Key Optimization Arguments</h3>
<div class="sect3">
<h4 id="_gpu_memory_utilization"><a class="anchor" href="#_gpu_memory_utilization"></a>--gpu-memory-utilization</h4>
<div class="paragraph">
<p><strong>Purpose</strong>: Controls how much GPU memory vLLM uses</p>
</div>
<div class="paragraph">
<p><strong>Recommended Value</strong>: <code>0.9</code> (90%)</p>
</div>
<div class="paragraph">
<p><strong>Impact:</strong>
- Higher value = more concurrent requests = higher throughput
- Lower value = safer but lower throughput
- 0.9 maximizes GPU utilization while leaving safety margin</p>
</div>
<div class="paragraph">
<p><strong>Example:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">--gpu-memory-utilization 0.9  # Use 90% of GPU memory</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_kv_cache_dtype"><a class="anchor" href="#_kv_cache_dtype"></a>--kv-cache-dtype</h4>
<div class="paragraph">
<p><strong>Purpose</strong>: Precision for KV cache storage</p>
</div>
<div class="paragraph">
<p><strong>Options:</strong>
- <code>auto</code>: Default, uses model precision
- <code>fp8</code>: 8-bit floating point (recommended)
- <code>fp16</code>: 16-bit floating point</p>
</div>
<div class="paragraph">
<p><strong>Recommended</strong>: <code>fp8</code> for memory efficiency</p>
</div>
<div class="paragraph">
<p><strong>Impact:</strong>
- FP8: 50% memory reduction, minimal accuracy impact
- Enables 2x more concurrent requests
- Better GPU utilization</p>
</div>
<div class="paragraph">
<p><strong>Example:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">--kv-cache-dtype fp8  # Use FP8 for KV cache</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_max_num_seqs"><a class="anchor" href="#_max_num_seqs"></a>--max-num-seqs</h4>
<div class="paragraph">
<p><strong>Purpose</strong>: Maximum number of concurrent sequences</p>
</div>
<div class="paragraph">
<p><strong>Default</strong>: 256</p>
</div>
<div class="paragraph">
<p><strong>Tuning:</strong>
- Higher = more concurrent requests = higher throughput
- Limited by GPU memory
- Start with default, increase if memory allows</p>
</div>
<div class="paragraph">
<p><strong>Calculation:</strong>
- GPU memory: 24GB
- Model: 8GB
- Available for KV cache: ~16GB
- Per request KV cache: ~2GB (8K context)
- Max sequences: ~8 (conservative) to 16 (aggressive)</p>
</div>
<div class="paragraph">
<p><strong>Example:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">--max-num-seqs 16  # Allow 16 concurrent sequences</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_tensor_parallel_size"><a class="anchor" href="#_tensor_parallel_size"></a>--tensor-parallel-size</h4>
<div class="paragraph">
<p><strong>Purpose</strong>: Number of GPUs for tensor parallelism</p>
</div>
<div class="paragraph">
<p><strong>Options:</strong>
- <code>1</code>: Single GPU (default)
- <code>2+</code>: Multi-GPU (requires multiple GPUs)</p>
</div>
<div class="paragraph">
<p><strong>Use Cases:</strong>
- Single GPU: Use <code>1</code>
- Multi-GPU node: Use number of GPUs
- Distributed: Use with pipeline parallelism</p>
</div>
<div class="paragraph">
<p><strong>Example:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">--tensor-parallel-size 1  # Single GPU
--tensor-parallel-size 2  # Two GPUs</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_max_model_len"><a class="anchor" href="#_max_model_len"></a>--max-model-len</h4>
<div class="paragraph">
<p><strong>Purpose</strong>: Maximum context length</p>
</div>
<div class="paragraph">
<p><strong>Default</strong>: Model-dependent</p>
</div>
<div class="paragraph">
<p><strong>Configuration:</strong>
- Set based on your use case
- Longer context = more memory per request
- Shorter context = more concurrent requests</p>
</div>
<div class="paragraph">
<p><strong>Example:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">--max-model-len 8192  # 8K context for Llama 3.1 8B</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_complete_optimized_configuration"><a class="anchor" href="#_complete_optimized_configuration"></a>Complete Optimized Configuration</h2>
<div class="sectionbody">
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: vllm-runtime-optimized
  namespace: lean-rag-accelerator
spec:
  supportedModelFormats:
    - name: llama
      version: "1"
      autoSelect: true
    - name: onnx
      version: "1"
      autoSelect: false

  multiModel: false
  grpcDataEndpoint: port:8001

  containers:
    - name: vllm
      image: vllm/vllm-openai:latest  # Update with your vLLM image
      imagePullPolicy: IfNotPresent

      args:
        - --model
        - /mnt/models
        - --host
        - "0.0.0.0"
        - --port
        - "8000"
        # Throughput optimizations:
        - --gpu-memory-utilization
        - "0.9"  # Maximize GPU usage
        - --kv-cache-dtype
        - "fp8"  # Memory-efficient KV cache
        - --max-num-seqs
        - "16"  # Concurrent sequences (tune based on memory)
        - --tensor-parallel-size
        - "1"  # Single GPU
        - --max-model-len
        - "8192"  # Context length
        - --trust-remote-code
        - --disable-log-requests  # Optional: disable for performance

      env:
        - name: CUDA_VISIBLE_DEVICES
          value: "0"

      ports:
        - containerPort: 8000
          name: http
          protocol: TCP
        - containerPort: 8001
          name: grpc
          protocol: TCP

      resources:
        requests:
          memory: "16Gi"
          cpu: "4"
          nvidia.com/gpu: 1
        limits:
          memory: "32Gi"
          cpu: "8"
          nvidia.com/gpu: 1

      volumeMounts:
        - name: model-storage
          mountPath: /mnt/models
          readOnly: true

  volumes:
    - name: model-storage
      persistentVolumeClaim:
        claimName: model-storage-pvc</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_deploying_vllm_runtime"><a class="anchor" href="#_deploying_vllm_runtime"></a>Deploying vLLM Runtime</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_step_1_deploy_servingruntime"><a class="anchor" href="#_step_1_deploy_servingruntime"></a>Step 1: Deploy ServingRuntime</h3>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">kubectl apply -f vllm-runtime.yaml</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_step_2_verify_deployment"><a class="anchor" href="#_step_2_verify_deployment"></a>Step 2: Verify Deployment</h3>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">kubectl get servingruntime vllm-runtime
kubectl describe servingruntime vllm-runtime</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_step_3_deploy_inferenceservice"><a class="anchor" href="#_step_3_deploy_inferenceservice"></a>Step 3: Deploy InferenceService</h3>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: lean-rag-inference
  namespace: lean-rag-accelerator
spec:
  predictor:
    model:
      storageUri: pvc://model-storage-pvc/models/llama-3.1-8b-int8
    runtime: vllm-runtime
    resources:
      limits:
        nvidia.com/gpu: 1</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">kubectl apply -f kserve-inferenceservice.yaml</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_step_4_monitor_deployment"><a class="anchor" href="#_step_4_monitor_deployment"></a>Step 4: Monitor Deployment</h3>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Watch service status
kubectl get inferenceservice lean-rag-inference -w

# Check pods
kubectl get pods -l serving.kserve.io/inferenceservice=lean-rag-inference

# Wait for Ready status (3/3)
kubectl get pods -l component=predictor</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_step_5_test_inference"><a class="anchor" href="#_step_5_test_inference"></a>Step 5: Test Inference</h3>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Get service endpoint
INGRESS_HOST=$(kubectl get inferenceservice lean-rag-inference -o jsonpath='{.status.url}')

# Test inference
curl -X POST $INGRESS_HOST/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "What is the capital of France?",
    "max_tokens": 50,
    "temperature": 0.7
  }'</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_performance_tuning"><a class="anchor" href="#_performance_tuning"></a>Performance Tuning</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_baseline_measurement"><a class="anchor" href="#_baseline_measurement"></a>Baseline Measurement</h3>
<div class="paragraph">
<p><strong>Measure before optimization:</strong>
- Throughput (requests/second)
- GPU utilization (%)
- Latency (TTFT, ITL)
- Memory usage</p>
</div>
</div>
<div class="sect2">
<h3 id="_iterative_tuning"><a class="anchor" href="#_iterative_tuning"></a>Iterative Tuning</h3>
<div class="paragraph">
<p><strong>Step 1: Start Conservative</strong>
- <code>--gpu-memory-utilization=0.7</code>
- <code>--max-num-seqs=8</code>
- Measure performance</p>
</div>
<div class="paragraph">
<p><strong>Step 2: Increase Gradually</strong>
- Increase to <code>0.8</code>, then <code>0.9</code>
- Increase <code>max-num-seqs</code> if memory allows
- Monitor for OOM errors</p>
</div>
<div class="paragraph">
<p><strong>Step 3: Optimize Further</strong>
- Enable FP8 KV cache
- Tune context length
- Test with production workload</p>
</div>
</div>
<div class="sect2">
<h3 id="_expected_results"><a class="anchor" href="#_expected_results"></a>Expected Results</h3>
<div class="paragraph">
<p><strong>Before Optimization:</strong>
- Throughput: 5 RPS
- GPU Utilization: 38%
- TTFT: 150ms</p>
</div>
<div class="paragraph">
<p><strong>After Optimization:</strong>
- Throughput: 15-20 RPS (3-4x)
- GPU Utilization: 85-90%
- TTFT: &lt;100ms</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_troubleshooting"><a class="anchor" href="#_troubleshooting"></a>Troubleshooting</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_gpu_out_of_memory_oom"><a class="anchor" href="#_gpu_out_of_memory_oom"></a>GPU Out of Memory (OOM)</h3>
<div class="paragraph">
<p><strong>Symptoms:</strong>
- Pod crashes with OOM error
- Cannot load model</p>
</div>
<div class="paragraph">
<p><strong>Solutions:</strong>
- Reduce <code>--gpu-memory-utilization</code> (try 0.7)
- Reduce <code>--max-num-seqs</code>
- Use quantized model (INT8)
- Use smaller context length</p>
</div>
</div>
<div class="sect2">
<h3 id="_low_throughput"><a class="anchor" href="#_low_throughput"></a>Low Throughput</h3>
<div class="paragraph">
<p><strong>Symptoms:</strong>
- Throughput lower than expected
- GPU utilization low</p>
</div>
<div class="paragraph">
<p><strong>Solutions:</strong>
- Increase <code>--gpu-memory-utilization</code> to 0.9
- Increase <code>--max-num-seqs</code>
- Enable FP8 KV cache
- Check for bottlenecks (network, storage)</p>
</div>
</div>
<div class="sect2">
<h3 id="_high_latency"><a class="anchor" href="#_high_latency"></a>High Latency</h3>
<div class="paragraph">
<p><strong>Symptoms:</strong>
- TTFT &gt;100ms
- Slow response times</p>
</div>
<div class="paragraph">
<p><strong>Solutions:</strong>
- Verify model is loaded correctly
- Check GPU availability
- Reduce batch size if too large
- Check for resource contention</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_key_takeaways"><a class="anchor" href="#_key_takeaways"></a>Key Takeaways</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>PagedAttention eliminates memory fragmentation (enabled by default)</p>
</li>
<li>
<p>Continuous Batching keeps GPU busy (enabled by default)</p>
</li>
<li>
<p><code>--gpu-memory-utilization=0.9</code> maximizes throughput</p>
</li>
<li>
<p><code>--kv-cache-dtype=fp8</code> reduces memory by 50%</p>
</li>
<li>
<p><code>--max-num-seqs</code> controls concurrent requests</p>
</li>
<li>
<p>2-4x throughput improvement is achievable</p>
</li>
<li>
<p>85-90% GPU utilization is the target</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_next_steps"><a class="anchor" href="#_next_steps"></a>Next Steps</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Learn about tuning vLLM engine arguments</p>
</li>
<li>
<p>Understand when to use llm-d for distributed inference</p>
</li>
<li>
<p>Explore prefix caching for RAG systems</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p><strong>Documentation Gaps</strong>: This section would benefit from:
- Visual diagram of PagedAttention memory layout
- Animation/video of continuous batching in action
- Performance tuning decision tree
- Real benchmark results with different configurations
- Screenshots of GPU utilization before/after</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="module-3-high-performance-serving-with-vllm-llm-d.html">Module 3: High-Performance Serving with vLLM &amp; llm-d</a></span>
  <span class="next"><a href="tuning-vllm-engine-arguments---gpu-memory-utilization-and---max-num-seqs.html">tuning vLLM engine arguments: <code>--gpu-memory-utilization</code> and <code>--max-num-seqs</code></a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
