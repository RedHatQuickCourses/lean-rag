<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Techniques for Model Compression: Quantization (FP8, INT8) and Sparsity :: Lean RAG Accelerator QuickStart Training</title>
    <link rel="prev" href="module-2-model-optimization-with-llm-compressor.html">
    <link rel="next" href="using-llm-compressor-to-create-optimized-modelcars-containerized-models.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">Lean RAG Accelerator QuickStart Training</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="lean-rag" data-version="1.0.0">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Lean RAG Accelerator QuickStart Training</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Home</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../LABENV/index.html">Lab Environment</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../module-1-inference-economics-rhoai-30-architecture/module-1-inference-economics-rhoai-30-architecture.html">Module 1: Inference Economics &amp; RHOAI 3.0 Architecture</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-1-inference-economics-rhoai-30-architecture/analysis-of-the-impossible-trinity-in-llm-deployments-balancing-accuracy-latency-and-cost.html">Analysis of the "Impossible Trinity" in LLM Deployments: Balancing Accuracy, Latency, and Cost</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-1-inference-economics-rhoai-30-architecture/identifying-critical-gpu-underutilization-and-the-role-of-memory-management-in-inference-costs.html">Identifying "Critical GPU Underutilization" and the Role of Memory Management in Inference Costs</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-1-inference-economics-rhoai-30-architecture/overview-of-the-red-hat-ai-30-ai-factory-approach-for-industrialized-model-serving.html">Overview of the Red Hat OpenShift AI 3.0 "AI Factory" Approach for Industrialized Model Serving</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-1-inference-economics-rhoai-30-architecture/mapping-the-rhoai-30-inference-stack-vllm-llm-d-and-kserve-integration.html">Mapping the RHOAI 3.0 inference stack: vLLM, llm-d, and KServe integration</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="module-2-model-optimization-with-llm-compressor.html">Module 2: Model Optimization with LLM Compressor</a>
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="techniques-for-model-compression-quantization-fp8-int8-and-sparsity.html">Techniques for Model Compression: Quantization (FP8, INT8) and Sparsity</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="using-llm-compressor-to-create-optimized-modelcars-containerized-models.html">Using LLM Compressor to Create Optimized Models</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="implementing-quantization-recipes-eg-smoothquant-gptq-to-reduce-gpu-memory-footprint.html">Implementing quantization recipes (e.g., SmoothQuant, GPTQ) to reduce GPU memory footprint</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="accessing-validated-pre-optimized-models-eg-lean-llama-from-the-red-hat-model-catalog.html">Accessing Validated, Pre-optimized Models from the Red Hat Model Catalog</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../module-3-high-performance-serving-with-vllm-llm-d/module-3-high-performance-serving-with-vllm-llm-d.html">Module 3: High-Performance Serving with vLLM &amp; llm-d</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-3-high-performance-serving-with-vllm-llm-d/configuring-vllm-for-maximum-throughput-using-pagedattention-and-continuous-batching.html">Configuring vLLM for Maximum Throughput Using PagedAttention and Continuous Batching</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-3-high-performance-serving-with-vllm-llm-d/tuning-vllm-engine-arguments---gpu-memory-utilization-and---max-num-seqs.html">tuning vLLM engine arguments: <code>--gpu-memory-utilization</code> and <code>--max-num-seqs</code></a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-3-high-performance-serving-with-vllm-llm-d/deploying-distributed-inference-with-llm-d-to-separate-prefill-and-decode-phases-disaggregation.html">Deploying distributed inference with llm-d to separate prefill and decode phases (disaggregation)</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-3-high-performance-serving-with-vllm-llm-d/implementing-prefix-caching-to-reuse-key-value-kv-cache-for-rag-system-prompts.html">Implementing prefix caching to reuse Key-Value (KV) cache for RAG system prompts</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../module-4-standardized-rag-implementation/module-4-standardized-rag-implementation.html">Module 4: Standardized RAG Implementation</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-4-standardized-rag-implementation/ingesting-and-chunking-unstructured-enterprise-data-pdfs-using-docling.html">Ingesting and Chunking Unstructured Enterprise Data using Docling</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-4-standardized-rag-implementation/orchestrating-rag-workflows-using-the-llama-stack-api-for-standardized-response-generation-and-vector-io.html">Orchestrating RAG Workflows with the Llama Stack API</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-4-standardized-rag-implementation/integrating-vector-databases-eg-milvus-chroma-via-llama-stack-providers.html">Integrating Vector Databases via Llama Stack Providers</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-4-standardized-rag-implementation/deploying-the-rag-application-stack-using-gitops-and-argocd-for-repeatability.html">deploying the RAG application stack using GitOps and ArgoCD for repeatability</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../module-5-validation-benchmarking/module-5-validation-benchmarking.html">Module 5: Validation &amp; Benchmarking</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-5-validation-benchmarking/defining-service-level-objectives-slos-for-time-to-first-token-ttft-and-inter-token-latency-itl.html">Defining Service Level Objectives (SLOs) for Time To First Token (TTFT) and Inter-Token Latency (ITL)</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-5-validation-benchmarking/simulating-real-world-traffic-load-using-guidellm-to-validate-throughput-gains.html">Simulating real-world traffic load using GuideLLM to validate throughput gains</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-5-validation-benchmarking/comparing-performance-metrics-throughput-vs-latency.html">Comparing performance metrics (throughput vs. latency</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../appendix/appendix.html">Appendix</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Lean RAG Accelerator QuickStart Training</span>
    <span class="version">1.0.0</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Lean RAG Accelerator QuickStart Training</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1.0.0</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Lean RAG Accelerator QuickStart Training</a></li>
    <li><a href="module-2-model-optimization-with-llm-compressor.html">Module 2: Model Optimization with LLM Compressor</a></li>
    <li><a href="techniques-for-model-compression-quantization-fp8-int8-and-sparsity.html">Techniques for Model Compression: Quantization (FP8, INT8) and Sparsity</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Techniques for Model Compression: Quantization (FP8, INT8) and Sparsity</h1>
<div class="sect1">
<h2 id="_introduction"><a class="anchor" href="#_introduction"></a>Introduction</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Model compression is essential for deploying LLMs in production. By reducing model size and memory requirements, you can:
- Serve models on smaller/cheaper GPUs
- Increase throughput (more concurrent requests)
- Reduce infrastructure costs by 50%+
- Maintain &gt;95% accuracy in most cases</p>
</div>
<div class="paragraph">
<p>This section covers quantization and sparsity techniques used in the Lean RAG Accelerator.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_understanding_quantization"><a class="anchor" href="#_understanding_quantization"></a>Understanding Quantization</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_what_is_quantization"><a class="anchor" href="#_what_is_quantization"></a>What is Quantization?</h3>
<div class="paragraph">
<p>Quantization converts high-precision floating-point numbers to lower-precision formats, reducing memory footprint and accelerating computation.</p>
</div>
<div class="paragraph">
<p><strong>Precision Levels:</strong>
- <strong>FP32</strong> (32-bit float): Full precision, baseline
- <strong>FP16</strong> (16-bit float): Half precision, 2x memory reduction
- <strong>FP8</strong> (8-bit float): Quarter precision, 4x memory reduction
- <strong>INT8</strong> (8-bit integer): Integer precision, 4x memory reduction
- <strong>INT4</strong> (4-bit integer): Very low precision, 8x memory reduction</p>
</div>
</div>
<div class="sect2">
<h3 id="_quantization_trade_offs"><a class="anchor" href="#_quantization_trade_offs"></a>Quantization Trade-offs</h3>
<div class="paragraph">
<p>| Precision | Memory Reduction | Speed Improvement | Accuracy Retention | Use Case |
|-----------|-----------------|-------------------|-------------------|----------|
| FP32 | Baseline | Baseline | 100% | Training, research |
| FP16 | 2x | 1.5x | ~99% | Standard inference |
| FP8 | 4x | 2x | ~98% | Production (new) |
| INT8 | 4x | 2x | ~95% | Production (common) |
| INT4 | 8x | 3x | ~90% | Resource-constrained |</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_quantization_methods"><a class="anchor" href="#_quantization_methods"></a>Quantization Methods</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_fp8_8_bit_floating_point"><a class="anchor" href="#_fp8_8_bit_floating_point"></a>FP8 (8-bit Floating Point)</h3>
<div class="paragraph">
<p><strong>Characteristics:</strong>
- Latest precision format (2023+)
- Maintains floating-point representation
- Better accuracy retention than INT8
- Hardware support in newer GPUs (H100, A100)</p>
</div>
<div class="paragraph">
<p><strong>Advantages:</strong>
- Best accuracy retention among 8-bit formats
- Natural fit for LLM activations
- Minimal calibration required</p>
</div>
<div class="paragraph">
<p><strong>Limitations:</strong>
- Requires newer GPU hardware
- Less widely supported than INT8</p>
</div>
<div class="paragraph">
<p><strong>Use Cases:</strong>
- Production deployments with modern GPUs
- When accuracy is critical
- New deployments (future-proof)</p>
</div>
<div class="paragraph">
<p><strong>Expected Results:</strong>
- 4x memory reduction
- 2x inference speed
- 98%+ accuracy retention</p>
</div>
</div>
<div class="sect2">
<h3 id="_int8_8_bit_integer"><a class="anchor" href="#_int8_8_bit_integer"></a>INT8 (8-bit Integer)</h3>
<div class="paragraph">
<p><strong>Characteristics:</strong>
- Most widely supported quantization format
- Integer representation (no floating point)
- Requires calibration dataset
- Well-established tooling</p>
</div>
<div class="paragraph">
<p><strong>Advantages:</strong>
- Broad hardware support
- Mature tooling (SmoothQuant, GPTQ)
- Excellent memory savings
- Good accuracy retention</p>
</div>
<div class="paragraph">
<p><strong>Limitations:</strong>
- Requires calibration dataset
- Slightly lower accuracy than FP8
- May need fine-tuning for some models</p>
</div>
<div class="paragraph">
<p><strong>Use Cases:</strong>
- Production deployments (most common)
- Cost-sensitive applications
- Standard GPU hardware</p>
</div>
<div class="paragraph">
<p><strong>Expected Results:</strong>
- 4x memory reduction
- 2x inference speed
- 95%+ accuracy retention</p>
</div>
<div class="paragraph">
<p><strong>Quantization Algorithms for INT8:</strong></p>
</div>
<div class="sect3">
<h4 id="_smoothquant"><a class="anchor" href="#_smoothquant"></a>SmoothQuant</h4>
<div class="paragraph">
<p><strong>How it works:</strong>
- Smooths activation outliers before quantization
- Migrates quantization difficulty from activations to weights
- Post-training quantization (no retraining needed)</p>
</div>
<div class="paragraph">
<p><strong>Advantages:</strong>
- No model retraining required
- Good accuracy retention
- Works well with most models</p>
</div>
<div class="paragraph">
<p><strong>Configuration:</strong>
- Alpha parameter (0.0-1.0): Controls smoothing
- Calibration dataset: 512-1024 samples typical
- Batch size: 8-16 for calibration</p>
</div>
<div class="paragraph">
<p><strong>Best for:</strong>
- General-purpose quantization
- Production deployments
- Llama models</p>
</div>
</div>
<div class="sect3">
<h4 id="_gptq_gpt_quantization"><a class="anchor" href="#_gptq_gpt_quantization"></a>GPTQ (GPT Quantization)</h4>
<div class="paragraph">
<p><strong>How it works:</strong>
- Layer-wise quantization with error correction
- More aggressive compression
- Typically used for INT4, but supports INT8</p>
</div>
<div class="paragraph">
<p><strong>Advantages:</strong>
- Maximum compression
- Good for resource-constrained environments
- Can achieve INT4 precision</p>
</div>
<div class="paragraph">
<p><strong>Limitations:</strong>
- More complex calibration
- Longer quantization time
- May require more calibration samples</p>
</div>
<div class="paragraph">
<p><strong>Best for:</strong>
- Maximum compression needs
- INT4 quantization
- Resource-constrained deployments</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_int4_4_bit_integer"><a class="anchor" href="#_int4_4_bit_integer"></a>INT4 (4-bit Integer)</h3>
<div class="paragraph">
<p><strong>Characteristics:</strong>
- Maximum compression (8x memory reduction)
- Significant accuracy trade-off
- Requires careful calibration</p>
</div>
<div class="paragraph">
<p><strong>Advantages:</strong>
- Maximum memory savings
- Can run large models on smaller GPUs
- Good for edge deployments</p>
</div>
<div class="paragraph">
<p><strong>Limitations:</strong>
- Lower accuracy retention (~90%)
- Requires extensive calibration
- May need model-specific tuning</p>
</div>
<div class="paragraph">
<p><strong>Use Cases:</strong>
- Resource-constrained environments
- Edge deployments
- When model size is critical</p>
</div>
<div class="paragraph">
<p><strong>Expected Results:</strong>
- 8x memory reduction
- 3x inference speed
- 90%+ accuracy retention</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_sparsity_techniques"><a class="anchor" href="#_sparsity_techniques"></a>Sparsity Techniques</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Sparsity introduces zero values into weight matrices, reducing the number of non-zero parameters.</p>
</div>
<div class="sect2">
<h3 id="_structured_sparsity"><a class="anchor" href="#_structured_sparsity"></a>Structured Sparsity</h3>
<div class="paragraph">
<p><strong>Approach:</strong>
- Prunes entire neurons, filters, or layers
- Results in regular, hardware-friendly patterns
- Easier to accelerate on specialized hardware</p>
</div>
<div class="paragraph">
<p><strong>Advantages:</strong>
- Hardware-friendly (efficient on GPUs)
- Predictable performance
- Easier to implement</p>
</div>
<div class="paragraph">
<p><strong>Limitations:</strong>
- Less flexible than unstructured
- May remove important structures
- Requires careful selection of what to prune</p>
</div>
<div class="paragraph">
<p><strong>Use Cases:</strong>
- Hardware-accelerated inference
- When regularity is important
- Production deployments</p>
</div>
</div>
<div class="sect2">
<h3 id="_unstructured_sparsity"><a class="anchor" href="#_unstructured_sparsity"></a>Unstructured Sparsity</h3>
<div class="paragraph">
<p><strong>Approach:</strong>
- Prunes individual weights within matrices
- Results in irregular, sparse patterns
- More flexible but harder to accelerate</p>
</div>
<div class="paragraph">
<p><strong>Advantages:</strong>
- Maximum flexibility
- Can achieve higher sparsity rates
- Better accuracy retention</p>
</div>
<div class="paragraph">
<p><strong>Limitations:</strong>
- Harder to accelerate (requires specialized hardware)
- Irregular patterns
- More complex implementation</p>
</div>
<div class="paragraph">
<p><strong>Use Cases:</strong>
- Research and experimentation
- When maximum sparsity is needed
- Specialized hardware deployments</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_combining_techniques"><a class="anchor" href="#_combining_techniques"></a>Combining Techniques</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_quantization_sparsity"><a class="anchor" href="#_quantization_sparsity"></a>Quantization + Sparsity</h3>
<div class="paragraph">
<p><strong>Strategy:</strong>
- Apply quantization first (INT8)
- Then apply sparsity (structured or unstructured)
- Can achieve 10x+ total compression</p>
</div>
<div class="paragraph">
<p><strong>Expected Results:</strong>
- 4x from quantization (INT8)
- 2-3x from sparsity
- Total: 8-12x compression
- Accuracy: 90-95% retention</p>
</div>
<div class="paragraph">
<p><strong>Use Cases:</strong>
- Extreme resource constraints
- Edge deployments
- Maximum cost optimization</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_real_world_examples"><a class="anchor" href="#_real_world_examples"></a>Real-World Examples</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_example_1_llama_3_1_8b_with_int8_smoothquant"><a class="anchor" href="#_example_1_llama_3_1_8b_with_int8_smoothquant"></a>Example 1: Llama 3.1 8B with INT8 SmoothQuant</h3>
<div class="paragraph">
<p><strong>Baseline:</strong>
- Model: Llama 3.1 8B FP16
- Size: ~16GB
- GPU: Requires A10G (24GB) or larger</p>
</div>
<div class="paragraph">
<p><strong>After INT8 SmoothQuant:</strong>
- Model: Llama 3.1 8B INT8
- Size: ~8GB (50% reduction)
- GPU: Can run on L4 (24GB) with room for batching
- Accuracy: 95%+ retention
- Speed: 2x faster inference</p>
</div>
</div>
<div class="sect2">
<h3 id="_example_2_llama_3_2_3b_with_int8"><a class="anchor" href="#_example_2_llama_3_2_3b_with_int8"></a>Example 2: Llama 3.2 3B with INT8</h3>
<div class="paragraph">
<p><strong>Baseline:</strong>
- Model: Llama 3.2 3B FP16
- Size: ~6GB</p>
</div>
<div class="paragraph">
<p><strong>After INT8:</strong>
- Model: Llama 3.2 3B INT8
- Size: ~3GB (50% reduction)
- Can run on CPU or smaller GPUs
- Good for edge deployments</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_choosing_the_right_technique"><a class="anchor" href="#_choosing_the_right_technique"></a>Choosing the Right Technique</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_decision_framework"><a class="anchor" href="#_decision_framework"></a>Decision Framework</h3>
<div class="paragraph">
<p><strong>Consider your requirements:</strong></p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Accuracy Requirements</strong></p>
<div class="ulist">
<ul>
<li>
<p>Critical (&gt;98%): Use FP8 or FP16</p>
</li>
<li>
<p>High (&gt;95%): Use INT8 SmoothQuant</p>
</li>
<li>
<p>Acceptable (&gt;90%): Use INT8 GPTQ or INT4</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Hardware Constraints</strong></p>
<div class="ulist">
<ul>
<li>
<p>Modern GPUs (H100, A100): FP8 available</p>
</li>
<li>
<p>Standard GPUs (A10G, L4): INT8 recommended</p>
</li>
<li>
<p>Resource-constrained: INT4 or INT8 + Sparsity</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Use Case</strong></p>
<div class="ulist">
<ul>
<li>
<p>Production RAG: INT8 SmoothQuant</p>
</li>
<li>
<p>Real-time chat: INT8 or FP8</p>
</li>
<li>
<p>Edge deployment: INT4 or INT8 + Sparsity</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Cost Sensitivity</strong></p>
<div class="ulist">
<ul>
<li>
<p>High: INT8 or INT4</p>
</li>
<li>
<p>Medium: FP8 or INT8</p>
</li>
<li>
<p>Low: FP16 (baseline)</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="_recommended_approach_for_lean_rag_accelerator"><a class="anchor" href="#_recommended_approach_for_lean_rag_accelerator"></a>Recommended Approach for Lean RAG Accelerator</h3>
<div class="paragraph">
<p><strong>For most RAG deployments:</strong>
- <strong>Method</strong>: INT8 SmoothQuant
- <strong>Model</strong>: Llama 3.1 8B
- <strong>Result</strong>: 50% memory reduction, 95%+ accuracy retention
- <strong>Implementation</strong>: LLM Compressor (see next section)</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_key_takeaways"><a class="anchor" href="#_key_takeaways"></a>Key Takeaways</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Quantization reduces memory by 2-8x with minimal accuracy loss</p>
</li>
<li>
<p>INT8 SmoothQuant is the recommended approach for most deployments</p>
</li>
<li>
<p>FP8 offers best accuracy but requires newer hardware</p>
</li>
<li>
<p>INT4 provides maximum compression but lower accuracy</p>
</li>
<li>
<p>Sparsity can be combined with quantization for extreme compression</p>
</li>
<li>
<p>95%+ accuracy retention is achievable with proper quantization</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_next_steps"><a class="anchor" href="#_next_steps"></a>Next Steps</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Learn how to use LLM Compressor to apply quantization</p>
</li>
<li>
<p>Understand quantization recipe configuration</p>
</li>
<li>
<p>See hands-on examples in Module 2 labs</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p><strong>Documentation Gaps</strong>: This section would benefit from:
- Visual diagram showing precision levels (FP32 â†’ INT4)
- Comparison charts of accuracy vs. memory for different methods
- Real benchmark results from quantized models
- Decision tree diagram for choosing quantization method</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="module-2-model-optimization-with-llm-compressor.html">Module 2: Model Optimization with LLM Compressor</a></span>
  <span class="next"><a href="using-llm-compressor-to-create-optimized-modelcars-containerized-models.html">Using LLM Compressor to Create Optimized Models</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
