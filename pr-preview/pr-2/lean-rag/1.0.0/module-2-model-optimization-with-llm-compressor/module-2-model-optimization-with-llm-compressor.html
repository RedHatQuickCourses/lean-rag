<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Module 2: Model Optimization with LLM Compressor :: Lean RAG Accelerator QuickStart Training</title>
    <link rel="prev" href="../module-1-inference-economics-rhoai-30-architecture/mapping-the-rhoai-30-inference-stack-vllm-llm-d-and-kserve-integration.html">
    <link rel="next" href="techniques-for-model-compression-quantization-fp8-int8-and-sparsity.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">Lean RAG Accelerator QuickStart Training</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="lean-rag" data-version="1.0.0">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Lean RAG Accelerator QuickStart Training</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Home</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../LABENV/index.html">Lab Environment</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../module-1-inference-economics-rhoai-30-architecture/module-1-inference-economics-rhoai-30-architecture.html">Module 1: Inference Economics &amp; RHOAI 3.0 Architecture</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-1-inference-economics-rhoai-30-architecture/analysis-of-the-impossible-trinity-in-llm-deployments-balancing-accuracy-latency-and-cost.html">Analysis of the "Impossible Trinity" in LLM Deployments: Balancing Accuracy, Latency, and Cost</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-1-inference-economics-rhoai-30-architecture/identifying-critical-gpu-underutilization-and-the-role-of-memory-management-in-inference-costs.html">Identifying "Critical GPU Underutilization" and the Role of Memory Management in Inference Costs</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-1-inference-economics-rhoai-30-architecture/overview-of-the-red-hat-ai-30-ai-factory-approach-for-industrialized-model-serving.html">Overview of the Red Hat OpenShift AI 3.0 "AI Factory" Approach for Industrialized Model Serving</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-1-inference-economics-rhoai-30-architecture/mapping-the-rhoai-30-inference-stack-vllm-llm-d-and-kserve-integration.html">Mapping the RHOAI 3.0 inference stack: vLLM, llm-d, and KServe integration</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="module-2-model-optimization-with-llm-compressor.html">Module 2: Model Optimization with LLM Compressor</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="techniques-for-model-compression-quantization-fp8-int8-and-sparsity.html">Techniques for Model Compression: Quantization (FP8, INT8) and Sparsity</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="using-llm-compressor-to-create-optimized-modelcars-containerized-models.html">Using LLM Compressor to Create Optimized Models</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="implementing-quantization-recipes-eg-smoothquant-gptq-to-reduce-gpu-memory-footprint.html">Implementing quantization recipes (e.g., SmoothQuant, GPTQ) to reduce GPU memory footprint</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="accessing-validated-pre-optimized-models-eg-lean-llama-from-the-red-hat-model-catalog.html">Accessing Validated, Pre-optimized Models from the Red Hat Model Catalog</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../module-3-high-performance-serving-with-vllm-llm-d/module-3-high-performance-serving-with-vllm-llm-d.html">Module 3: High-Performance Serving with vLLM &amp; llm-d</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-3-high-performance-serving-with-vllm-llm-d/configuring-vllm-for-maximum-throughput-using-pagedattention-and-continuous-batching.html">Configuring vLLM for Maximum Throughput Using PagedAttention and Continuous Batching</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-3-high-performance-serving-with-vllm-llm-d/tuning-vllm-engine-arguments---gpu-memory-utilization-and---max-num-seqs.html">tuning vLLM engine arguments: <code>--gpu-memory-utilization</code> and <code>--max-num-seqs</code></a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-3-high-performance-serving-with-vllm-llm-d/deploying-distributed-inference-with-llm-d-to-separate-prefill-and-decode-phases-disaggregation.html">Deploying distributed inference with llm-d to separate prefill and decode phases (disaggregation)</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-3-high-performance-serving-with-vllm-llm-d/implementing-prefix-caching-to-reuse-key-value-kv-cache-for-rag-system-prompts.html">Implementing prefix caching to reuse Key-Value (KV) cache for RAG system prompts</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../module-4-standardized-rag-implementation/module-4-standardized-rag-implementation.html">Module 4: Standardized RAG Implementation</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-4-standardized-rag-implementation/ingesting-and-chunking-unstructured-enterprise-data-pdfs-using-docling.html">Ingesting and Chunking Unstructured Enterprise Data using Docling</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-4-standardized-rag-implementation/orchestrating-rag-workflows-using-the-llama-stack-api-for-standardized-response-generation-and-vector-io.html">Orchestrating RAG Workflows with the Llama Stack API</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-4-standardized-rag-implementation/integrating-vector-databases-eg-milvus-chroma-via-llama-stack-providers.html">Integrating Vector Databases via Llama Stack Providers</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-4-standardized-rag-implementation/deploying-the-rag-application-stack-using-gitops-and-argocd-for-repeatability.html">deploying the RAG application stack using GitOps and ArgoCD for repeatability</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../module-5-validation-benchmarking/module-5-validation-benchmarking.html">Module 5: Validation &amp; Benchmarking</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-5-validation-benchmarking/defining-service-level-objectives-slos-for-time-to-first-token-ttft-and-inter-token-latency-itl.html">Defining Service Level Objectives (SLOs) for Time To First Token (TTFT) and Inter-Token Latency (ITL)</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-5-validation-benchmarking/simulating-real-world-traffic-load-using-guidellm-to-validate-throughput-gains.html">Simulating real-world traffic load using GuideLLM to validate throughput gains</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-5-validation-benchmarking/comparing-performance-metrics-throughput-vs-latency.html">Comparing performance metrics (throughput vs. latency</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../appendix/appendix.html">Appendix</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Lean RAG Accelerator QuickStart Training</span>
    <span class="version">1.0.0</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Lean RAG Accelerator QuickStart Training</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1.0.0</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Lean RAG Accelerator QuickStart Training</a></li>
    <li><a href="module-2-model-optimization-with-llm-compressor.html">Module 2: Model Optimization with LLM Compressor</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Module 2: Model Optimization with LLM Compressor</h1>
<div class="sect1">
<h2 id="_introduction"><a class="anchor" href="#_introduction"></a>Introduction</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This module covers how to optimize LLM models using RHOAI 3.0&#8217;s LLM Compressor to reduce memory footprint by 50-75% while maintaining &gt;95% accuracy. You&#8217;ll learn quantization techniques, how to use LLM Compressor, and how to access pre-optimized models from the Red Hat Model Catalog.</p>
</div>
<div class="paragraph">
<p><strong>Key Learning Objectives:</strong>
- Understand quantization techniques (FP8, INT8, INT4) and trade-offs
- Use LLM Compressor to create optimized models
- Implement SmoothQuant and GPTQ quantization recipes
- Access pre-optimized models from Red Hat Model Catalog</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_module_overview"><a class="anchor" href="#_module_overview"></a>Module Overview</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Model optimization is the first step in the Lean RAG Accelerator workflow. By reducing model size through quantization, you can:
- Serve models on smaller/cheaper GPUs
- Increase throughput (more concurrent requests)
- Reduce infrastructure costs by 50%+
- Maintain &gt;95% accuracy in most cases</p>
</div>
<div class="sect2">
<h3 id="_the_optimization_process"><a class="anchor" href="#_the_optimization_process"></a>The Optimization Process</h3>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Choose Quantization Method</strong>: Select FP8, INT8, or INT4 based on requirements</p>
</li>
<li>
<p><strong>Configure Recipe</strong>: Define QuantizationRecipe with model source and parameters</p>
</li>
<li>
<p><strong>Run Optimization</strong>: Execute quantization job (30-60 minutes)</p>
</li>
<li>
<p><strong>Deploy Optimized Model</strong>: Use optimized model in inference serving</p>
</li>
</ol>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_topics_covered"><a class="anchor" href="#_topics_covered"></a>Topics Covered</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_section_2_1_model_compression_techniques"><a class="anchor" href="#_section_2_1_model_compression_techniques"></a>Section 2.1: Model Compression Techniques</h3>
<div class="paragraph">
<p>Learn about quantization and sparsity techniques:
- Quantization fundamentals (FP32 â†’ INT4)
- FP8, INT8, INT4 comparison
- SmoothQuant and GPTQ algorithms
- Sparsity techniques
- Decision framework</p>
</div>
<div class="paragraph">
<p><strong>Duration</strong>: 60-75 minutes</p>
</div>
</div>
<div class="sect2">
<h3 id="_section_2_2_using_llm_compressor"><a class="anchor" href="#_section_2_2_using_llm_compressor"></a>Section 2.2: Using LLM Compressor</h3>
<div class="paragraph">
<p>Hands-on guide to using RHOAI 3.0&#8217;s LLM Compressor:
- QuantizationRecipe CRD structure
- Step-by-step configuration
- Deployment and monitoring
- Troubleshooting</p>
</div>
<div class="paragraph">
<p><strong>Duration</strong>: 75-90 minutes</p>
</div>
</div>
<div class="sect2">
<h3 id="_section_2_3_implementing_quantization_recipes"><a class="anchor" href="#_section_2_3_implementing_quantization_recipes"></a>Section 2.3: Implementing Quantization Recipes</h3>
<div class="paragraph">
<p>Practical implementation of quantization:
- SmoothQuant configuration
- GPTQ configuration
- Advanced parameters
- Best practices</p>
</div>
<div class="paragraph">
<p><strong>Duration</strong>: 90-120 minutes</p>
</div>
</div>
<div class="sect2">
<h3 id="_section_2_4_red_hat_model_catalog"><a class="anchor" href="#_section_2_4_red_hat_model_catalog"></a>Section 2.4: Red Hat Model Catalog</h3>
<div class="paragraph">
<p>Access pre-optimized models:
- Model Catalog overview
- Access methods (Hugging Face, OCI, ModelCars)
- Model selection guide
- Using catalog models</p>
</div>
<div class="paragraph">
<p><strong>Duration</strong>: 45-60 minutes</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_prerequisites"><a class="anchor" href="#_prerequisites"></a>Prerequisites</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Before starting this module, ensure you have:
- Completed Module 1 (understanding inference economics)
- Red Hat OpenShift AI 3.0 installed
- Access to GPU resources for quantization
- Basic Kubernetes knowledge</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_expected_outcomes"><a class="anchor" href="#_expected_outcomes"></a>Expected Outcomes</h2>
<div class="sectionbody">
<div class="paragraph">
<p>By the end of this module, you will be able to:
- Choose appropriate quantization methods
- Create and deploy QuantizationRecipe
- Monitor quantization jobs
- Access and use models from Model Catalog
- Validate quantization results</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_module_structure"><a class="anchor" href="#_module_structure"></a>Module Structure</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This module follows a hands-on approach:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Concept Learning</strong>: Understand quantization techniques</p>
</li>
<li>
<p><strong>Tool Usage</strong>: Learn LLM Compressor</p>
</li>
<li>
<p><strong>Implementation</strong>: Create quantization recipes</p>
</li>
<li>
<p><strong>Catalog Access</strong>: Use pre-optimized models</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_next_steps"><a class="anchor" href="#_next_steps"></a>Next Steps</h2>
<div class="sectionbody">
<div class="paragraph">
<p>After completing this module:
- Proceed to Module 3 to learn about high-performance serving
- Deploy your optimized model using vLLM
- Compare performance before and after optimization</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_related_resources"><a class="anchor" href="#_related_resources"></a>Related Resources</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p><a href="module-1-inference-economics-rhoai-30-architecture.adoc">Module 1: Inference Economics</a> - Previous module</p>
</li>
<li>
<p><a href="module-3-high-performance-serving-with-vllm-llm-d.adoc">Module 3: High-Performance Serving</a> - Next module</p>
</li>
<li>
<p><a href="../../lean-rag-accelerator/examples/01-model-optimization/README.md">Model Optimization Examples</a> - Quickstart code</p>
</li>
</ul>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="../module-1-inference-economics-rhoai-30-architecture/mapping-the-rhoai-30-inference-stack-vllm-llm-d-and-kserve-integration.html">Mapping the RHOAI 3.0 inference stack: vLLM, llm-d, and KServe integration</a></span>
  <span class="next"><a href="techniques-for-model-compression-quantization-fp8-int8-and-sparsity.html">Techniques for Model Compression: Quantization (FP8, INT8) and Sparsity</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
