<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Accessing Validated, Pre-optimized Models from the Red Hat Model Catalog :: Lean RAG Accelerator QuickStart Training</title>
    <link rel="prev" href="implementing-quantization-recipes-eg-smoothquant-gptq-to-reduce-gpu-memory-footprint.html">
    <link rel="next" href="../module-3-high-performance-serving-with-vllm-llm-d/module-3-high-performance-serving-with-vllm-llm-d.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">Lean RAG Accelerator QuickStart Training</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="lean-rag" data-version="1.0.0">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Lean RAG Accelerator QuickStart Training</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Home</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../LABENV/index.html">Lab Environment</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../module-1-inference-economics-rhoai-30-architecture/module-1-inference-economics-rhoai-30-architecture.html">Module 1: Inference Economics &amp; RHOAI 3.0 Architecture</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-1-inference-economics-rhoai-30-architecture/analysis-of-the-impossible-trinity-in-llm-deployments-balancing-accuracy-latency-and-cost.html">Analysis of the "Impossible Trinity" in LLM Deployments: Balancing Accuracy, Latency, and Cost</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-1-inference-economics-rhoai-30-architecture/identifying-critical-gpu-underutilization-and-the-role-of-memory-management-in-inference-costs.html">Identifying "Critical GPU Underutilization" and the Role of Memory Management in Inference Costs</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-1-inference-economics-rhoai-30-architecture/overview-of-the-red-hat-ai-30-ai-factory-approach-for-industrialized-model-serving.html">Overview of the Red Hat OpenShift AI 3.0 "AI Factory" Approach for Industrialized Model Serving</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-1-inference-economics-rhoai-30-architecture/mapping-the-rhoai-30-inference-stack-vllm-llm-d-and-kserve-integration.html">Mapping the RHOAI 3.0 inference stack: vLLM, llm-d, and KServe integration</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="module-2-model-optimization-with-llm-compressor.html">Module 2: Model Optimization with LLM Compressor</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="techniques-for-model-compression-quantization-fp8-int8-and-sparsity.html">Techniques for Model Compression: Quantization (FP8, INT8) and Sparsity</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="using-llm-compressor-to-create-optimized-modelcars-containerized-models.html">Using LLM Compressor to Create Optimized Models</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="implementing-quantization-recipes-eg-smoothquant-gptq-to-reduce-gpu-memory-footprint.html">Implementing quantization recipes (e.g., SmoothQuant, GPTQ) to reduce GPU memory footprint</a>
  </li>
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="accessing-validated-pre-optimized-models-eg-lean-llama-from-the-red-hat-model-catalog.html">Accessing Validated, Pre-optimized Models from the Red Hat Model Catalog</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../module-3-high-performance-serving-with-vllm-llm-d/module-3-high-performance-serving-with-vllm-llm-d.html">Module 3: High-Performance Serving with vLLM &amp; llm-d</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-3-high-performance-serving-with-vllm-llm-d/configuring-vllm-for-maximum-throughput-using-pagedattention-and-continuous-batching.html">Configuring vLLM for Maximum Throughput Using PagedAttention and Continuous Batching</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-3-high-performance-serving-with-vllm-llm-d/tuning-vllm-engine-arguments---gpu-memory-utilization-and---max-num-seqs.html">tuning vLLM engine arguments: <code>--gpu-memory-utilization</code> and <code>--max-num-seqs</code></a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-3-high-performance-serving-with-vllm-llm-d/deploying-distributed-inference-with-llm-d-to-separate-prefill-and-decode-phases-disaggregation.html">Deploying distributed inference with llm-d to separate prefill and decode phases (disaggregation)</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-3-high-performance-serving-with-vllm-llm-d/implementing-prefix-caching-to-reuse-key-value-kv-cache-for-rag-system-prompts.html">Implementing prefix caching to reuse Key-Value (KV) cache for RAG system prompts</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../module-4-standardized-rag-implementation/module-4-standardized-rag-implementation.html">Module 4: Standardized RAG Implementation</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-4-standardized-rag-implementation/ingesting-and-chunking-unstructured-enterprise-data-pdfs-using-docling.html">Ingesting and Chunking Unstructured Enterprise Data using Docling</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-4-standardized-rag-implementation/orchestrating-rag-workflows-using-the-llama-stack-api-for-standardized-response-generation-and-vector-io.html">Orchestrating RAG Workflows with the Llama Stack API</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-4-standardized-rag-implementation/integrating-vector-databases-eg-milvus-chroma-via-llama-stack-providers.html">Integrating Vector Databases via Llama Stack Providers</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-4-standardized-rag-implementation/deploying-the-rag-application-stack-using-gitops-and-argocd-for-repeatability.html">deploying the RAG application stack using GitOps and ArgoCD for repeatability</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../module-5-validation-benchmarking/module-5-validation-benchmarking.html">Module 5: Validation &amp; Benchmarking</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-5-validation-benchmarking/defining-service-level-objectives-slos-for-time-to-first-token-ttft-and-inter-token-latency-itl.html">Defining Service Level Objectives (SLOs) for Time To First Token (TTFT) and Inter-Token Latency (ITL)</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-5-validation-benchmarking/simulating-real-world-traffic-load-using-guidellm-to-validate-throughput-gains.html">Simulating real-world traffic load using GuideLLM to validate throughput gains</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../module-5-validation-benchmarking/comparing-performance-metrics-throughput-vs-latency.html">Comparing performance metrics (throughput vs. latency</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../appendix/appendix.html">Appendix</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Lean RAG Accelerator QuickStart Training</span>
    <span class="version">1.0.0</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Lean RAG Accelerator QuickStart Training</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1.0.0</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Lean RAG Accelerator QuickStart Training</a></li>
    <li><a href="module-2-model-optimization-with-llm-compressor.html">Module 2: Model Optimization with LLM Compressor</a></li>
    <li><a href="accessing-validated-pre-optimized-models-eg-lean-llama-from-the-red-hat-model-catalog.html">Accessing Validated, Pre-optimized Models from the Red Hat Model Catalog</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Accessing Validated, Pre-optimized Models from the Red Hat Model Catalog</h1>
<div class="sect1">
<h2 id="_introduction"><a class="anchor" href="#_introduction"></a>Introduction</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The Red Hat Model Catalog provides validated, pre-optimized models for use with Red Hat AI Inference Server (RHAIIS) and RHOAI 3.0. These models are available in multiple formats and quantization variants (INT4, INT8, FP8), making it easy to deploy optimized models without custom quantization.</p>
</div>
<div class="paragraph">
<p><strong>Key Benefits:</strong>
- Pre-validated models tested with RHOAI 3.0
- Multiple quantization variants available
- OCI container images for easy deployment
- ModelCar images for OpenShift AI
- Hugging Face access for development</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_model_catalog_access_methods"><a class="anchor" href="#_model_catalog_access_methods"></a>Model Catalog Access Methods</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_method_1_hugging_face"><a class="anchor" href="#_method_1_hugging_face"></a>Method 1: Hugging Face</h3>
<div class="paragraph">
<p>Models validated for use with Red Hat AI Inference Server are available from the <strong>RedHat AI on Hugging Face</strong> collection.</p>
</div>
<div class="paragraph">
<p><strong>Access:</strong>
- Collection: <code>RedHatAI</code> on Hugging Face
- Format: Standard Hugging Face model format
- Use case: Development and testing</p>
</div>
<div class="paragraph">
<p><strong>Example:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  model:
    source: hf://RedHatAI/Qwen3-8B-FP8-dynamic</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_method_2_oci_artifacts_rhel_ai"><a class="anchor" href="#_method_2_oci_artifacts_rhel_ai"></a>Method 2: OCI Artifacts (RHEL AI)</h3>
<div class="paragraph">
<p>For RHEL AI deployments, use OCI artifact images from the <code>registry.redhat.io/rhelai1/</code> namespace.</p>
</div>
<div class="paragraph">
<p><strong>Registry</strong>: <code>registry.redhat.io/rhelai1/</code></p>
</div>
<div class="paragraph">
<p><strong>Example Models:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml"># Baseline Llama-3-1-8B-Instruct
spec:
  model:
    source: oci://registry.redhat.io/rhelai1/llama-3-1-8b-instruct:1.5

# INT4 Llama-4-Scout-17B (quantized)
spec:
  model:
    source: oci://registry.redhat.io/rhelai1/llama-4-scout-17b-16e-instruct-quantized-w4a16:1.5</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_method_3_modelcar_images_openshift_ai"><a class="anchor" href="#_method_3_modelcar_images_openshift_ai"></a>Method 3: ModelCar Images (OpenShift AI)</h3>
<div class="paragraph">
<p>For OpenShift AI deployments, use ModelCar images from the <code>registry.redhat.io/rhelai1/modelcar-</code> namespace.</p>
</div>
<div class="paragraph">
<p><strong>Registry</strong>: <code>registry.redhat.io/rhelai1/modelcar-*</code></p>
</div>
<div class="paragraph">
<p><strong>Example Models:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml"># Baseline granite-3-1-8b-instruct
spec:
  model:
    source: oci://registry.redhat.io/rhelai1/modelcar-granite-3-1-8b-instruct:1.5

# FP8 Mistral-Small-3-1-24B-Instruct-2503
spec:
  model:
    source: oci://registry.redhat.io/rhelai1/modelcar-mistral-small-3-1-24b-instruct-2503-fp8-dynamic:1.5</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p><strong>ModelCars</strong>: ModelCar images are OCI container images containing pre-packaged models. They reduce startup times and disk usage by allowing pre-fetched images. ModelCars are optimized for OpenShift AI deployments.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_available_model_variants"><a class="anchor" href="#_available_model_variants"></a>Available Model Variants</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_quantization_variants"><a class="anchor" href="#_quantization_variants"></a>Quantization Variants</h3>
<div class="paragraph">
<p>Models in the catalog are available in multiple quantization variants:</p>
</div>
<div class="paragraph">
<p>| Variant | Description | Use Case |
|---------|-------------|----------|
| <strong>Baseline</strong> | Full precision (FP16/FP32) | Maximum accuracy |
| <strong>INT4</strong> | 4-bit integer quantization | Maximum compression |
| <strong>INT8</strong> | 8-bit integer quantization | Balanced performance |
| <strong>FP8</strong> | 8-bit floating point | Best accuracy retention |</p>
</div>
</div>
<div class="sect2">
<h3 id="_example_model_catalog"><a class="anchor" href="#_example_model_catalog"></a>Example Model Catalog</h3>
<div class="paragraph">
<p><strong>Baseline Models:</strong>
- <code>llama-3-1-8b-instruct:1.5</code> (OCI artifact)
- <code>modelcar-granite-3-1-8b-instruct:1.5</code> (ModelCar)</p>
</div>
<div class="paragraph">
<p><strong>Quantized Models:</strong>
- <code>llama-4-scout-17b-16e-instruct-quantized-w4a16:1.5</code> (INT4, OCI artifact)
- <code>modelcar-mistral-small-3-1-24b-instruct-2503-fp8-dynamic:1.5</code> (FP8, ModelCar)</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_using_models_from_catalog"><a class="anchor" href="#_using_models_from_catalog"></a>Using Models from Catalog</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_in_llminferenceservice"><a class="anchor" href="#_in_llminferenceservice"></a>In LLMInferenceService</h3>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: llama.redhat.com/v1alpha1
kind: LLMInferenceService
metadata:
  name: lean-rag-inference
spec:
  model:
    # Option 1: Hugging Face
    source: hf://RedHatAI/Qwen3-8B-FP8-dynamic

    # Option 2: OCI Artifact (RHEL AI)
    # source: oci://registry.redhat.io/rhelai1/llama-3-1-8b-instruct:1.5

    # Option 3: ModelCar (OpenShift AI)
    # source: oci://registry.redhat.io/rhelai1/modelcar-granite-3-1-8b-instruct:1.5

    name: RedHatAI/Qwen3-8B-FP8-dynamic</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_in_quantizationrecipe_for_custom_optimization"><a class="anchor" href="#_in_quantizationrecipe_for_custom_optimization"></a>In QuantizationRecipe (for custom optimization)</h3>
<div class="paragraph">
<p>If you want to further optimize a catalog model, you can reference it in a QuantizationRecipe:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: ai.redhat.com/v1alpha1
kind: QuantizationRecipe
metadata:
  name: optimize-catalog-model
spec:
  model:
    source:
      # Reference catalog model
      uri: oci://registry.redhat.io/rhelai1/llama-3-1-8b-instruct:1.5
  quantization:
    method: smoothquant
    targetPrecision: int8
  # ... rest of configuration</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_model_selection_guide"><a class="anchor" href="#_model_selection_guide"></a>Model Selection Guide</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_choose_baseline_when"><a class="anchor" href="#_choose_baseline_when"></a>Choose Baseline When:</h3>
<div class="ulist">
<ul>
<li>
<p>Maximum accuracy is required</p>
</li>
<li>
<p>You have sufficient GPU memory</p>
</li>
<li>
<p>You can accept higher costs</p>
</li>
<li>
<p>Use case: Research, high-stakes applications</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_choose_int8_when"><a class="anchor" href="#_choose_int8_when"></a>Choose INT8 When:</h3>
<div class="ulist">
<ul>
<li>
<p>Good balance of accuracy and efficiency</p>
</li>
<li>
<p>Standard production deployments</p>
</li>
<li>
<p>50% memory reduction needed</p>
</li>
<li>
<p>Use case: Most production RAG systems</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_choose_int4_when"><a class="anchor" href="#_choose_int4_when"></a>Choose INT4 When:</h3>
<div class="ulist">
<ul>
<li>
<p>Maximum compression needed</p>
</li>
<li>
<p>Resource-constrained environments</p>
</li>
<li>
<p>Can accept ~90% accuracy retention</p>
</li>
<li>
<p>Use case: Edge deployments, cost-sensitive</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_choose_fp8_when"><a class="anchor" href="#_choose_fp8_when"></a>Choose FP8 When:</h3>
<div class="ulist">
<ul>
<li>
<p>Best accuracy retention among 8-bit formats</p>
</li>
<li>
<p>Modern GPU hardware available</p>
</li>
<li>
<p>Production deployments</p>
</li>
<li>
<p>Use case: New deployments, future-proof</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_best_practices"><a class="anchor" href="#_best_practices"></a>Best Practices</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_model_versioning"><a class="anchor" href="#_model_versioning"></a>Model Versioning</h3>
<div class="ulist">
<ul>
<li>
<p>Always specify version tags (e.g., <code>:1.5</code>)</p>
</li>
<li>
<p>Pin versions for production deployments</p>
</li>
<li>
<p>Test new versions before upgrading</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_registry_authentication"><a class="anchor" href="#_registry_authentication"></a>Registry Authentication</h3>
<div class="paragraph">
<p><strong>For Red Hat Registry:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Login to Red Hat Registry
podman login registry.redhat.io
# Or
docker login registry.redhat.io</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Credentials:</strong>
- Use Red Hat account credentials
- Or service account with registry access</p>
</div>
</div>
<div class="sect2">
<h3 id="_model_caching"><a class="anchor" href="#_model_caching"></a>Model Caching</h3>
<div class="paragraph">
<p><strong>ModelCars</strong> provide built-in caching:
- Pre-fetched images reduce startup time
- Shared layers reduce disk usage
- Faster pod startup in Kubernetes</p>
</div>
</div>
<div class="sect2">
<h3 id="_performance_considerations"><a class="anchor" href="#_performance_considerations"></a>Performance Considerations</h3>
<div class="ulist">
<ul>
<li>
<p><strong>ModelCar images</strong>: Faster startup, less disk usage</p>
</li>
<li>
<p><strong>OCI artifacts</strong>: Standard format, more flexible</p>
</li>
<li>
<p><strong>Hugging Face</strong>: Easy development, may be slower for production</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_troubleshooting"><a class="anchor" href="#_troubleshooting"></a>Troubleshooting</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_registry_access_issues"><a class="anchor" href="#_registry_access_issues"></a>Registry Access Issues</h3>
<div class="paragraph">
<p><strong>Problem</strong>: Cannot pull model from registry</p>
</div>
<div class="paragraph">
<p><strong>Solutions:</strong>
- Verify registry authentication
- Check network connectivity
- Verify model name and version exist
- Check registry credentials in Kubernetes secrets</p>
</div>
</div>
<div class="sect2">
<h3 id="_model_not_found"><a class="anchor" href="#_model_not_found"></a>Model Not Found</h3>
<div class="paragraph">
<p><strong>Problem</strong>: Model URI not recognized</p>
</div>
<div class="paragraph">
<p><strong>Solutions:</strong>
- Verify exact model name and version
- Check registry namespace (<code>rhelai1/</code> for OCI, <code>rhelai1/modelcar-</code> for ModelCars)
- Verify model exists in catalog
- Check URI format (oci://, hf://)</p>
</div>
</div>
<div class="sect2">
<h3 id="_version_mismatch"><a class="anchor" href="#_version_mismatch"></a>Version Mismatch</h3>
<div class="paragraph">
<p><strong>Problem</strong>: Model version incompatible</p>
</div>
<div class="paragraph">
<p><strong>Solutions:</strong>
- Check RHOAI 3.0 compatibility
- Verify model version supports your use case
- Review model documentation
- Test with different versions</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_key_takeaways"><a class="anchor" href="#_key_takeaways"></a>Key Takeaways</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Red Hat Model Catalog provides validated, pre-optimized models</p>
</li>
<li>
<p>Three access methods: Hugging Face, OCI artifacts, ModelCars</p>
</li>
<li>
<p>Multiple quantization variants available (INT4, INT8, FP8, Baseline)</p>
</li>
<li>
<p>ModelCars optimize for OpenShift AI deployments</p>
</li>
<li>
<p>Always specify version tags for reproducibility</p>
</li>
<li>
<p>Choose model variant based on accuracy/efficiency trade-offs</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_next_steps"><a class="anchor" href="#_next_steps"></a>Next Steps</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Learn about custom model optimization with LLM Compressor</p>
</li>
<li>
<p>Understand how to deploy models using ModelCars</p>
</li>
<li>
<p>Explore quantization recipes for further optimization</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p><strong>Documentation Gaps</strong>: This section would benefit from:
- Complete list of available models in catalog
- Model compatibility matrix
- Performance benchmarks for catalog models
- Model metadata and specifications
- Catalog browsing interface screenshots</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="implementing-quantization-recipes-eg-smoothquant-gptq-to-reduce-gpu-memory-footprint.html">Implementing quantization recipes (e.g., SmoothQuant, GPTQ) to reduce GPU memory footprint</a></span>
  <span class="next"><a href="../module-3-high-performance-serving-with-vllm-llm-d/module-3-high-performance-serving-with-vllm-llm-d.html">Module 3: High-Performance Serving with vLLM &amp; llm-d</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
